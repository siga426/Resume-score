import os
import io
import json
import tempfile
import sys
from datetime import datetime
from typing import List
from contextlib import contextmanager

import streamlit as st
import pandas as pd

from resume_extractor import ResumeExtractor
from query_loader import QueryLoader


# åˆ›å»ºæ—¥å¿—æ•è·å™¨ç±»
class StreamlitLogCapture:
    """æ•è·printè¾“å‡ºå¹¶æ˜¾ç¤ºåœ¨Streamlitç•Œé¢ä¸Šçš„æ—¥å¿—æ•è·å™¨"""
    
    def __init__(self, container):
        self.container = container
        self.original_stdout = sys.stdout
        self.log_buffer = []
        self.max_logs = 200  # æœ€å¤§æ—¥å¿—æ¡æ•°
        
    def write(self, text):
        """é‡å†™stdoutçš„writeæ–¹æ³•"""
        if text.strip():  # åªå¤„ç†éç©ºæ–‡æœ¬
            # ä¿å­˜åˆ°åŸå§‹stdout
            self.original_stdout.write(text)
            self.original_stdout.flush()
            
            # æ·»åŠ åˆ°æ—¥å¿—ç¼“å†²åŒº
            timestamp = datetime.now().strftime('%H:%M:%S')
            log_entry = f"[{timestamp}] {text.rstrip()}"
            self.log_buffer.append(log_entry)
            
            # é™åˆ¶æ—¥å¿—æ¡æ•°
            if len(self.log_buffer) > self.max_logs:
                self.log_buffer = self.log_buffer[-self.max_logs:]
    
    def flush(self):
        """é‡å†™stdoutçš„flushæ–¹æ³•"""
        self.original_stdout.flush()
    
    def update_display(self):
        """æ›´æ–°Streamlitæ—¥å¿—æ˜¾ç¤º"""
        try:
            # æ¸…ç©ºå®¹å™¨å¹¶é‡æ–°æ˜¾ç¤ºæ—¥å¿—
            self.container.empty()
            with self.container:
                st.subheader("ğŸ“‹ å®æ—¶æ‰§è¡Œæ—¥å¿—")
                st.caption("æ˜¾ç¤ºç¨‹åºæ‰§è¡Œè¿‡ç¨‹ä¸­çš„printè¾“å‡º")
                
                # æ˜¾ç¤ºæ—¥å¿—å†…å®¹
                if self.log_buffer:
                    log_text = "\n".join(self.log_buffer)
                    st.code(log_text, language="text")
                    
                    # æ˜¾ç¤ºæ—¥å¿—ç»Ÿè®¡ä¿¡æ¯
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("æ—¥å¿—æ¡æ•°", len(self.log_buffer))
                    with col2:
                        if self.log_buffer:
                            last_log_time = self.log_buffer[-1].split(']')[0].replace('[', '')
                            st.metric("æœ€åæ›´æ–°", last_log_time)
                    with col3:
                        st.metric("ç¼“å†²åŒºå¤§å°", f"{len(self.log_buffer)}/{self.max_logs}")
                    
                    # æ·»åŠ æ“ä½œæŒ‰é’®
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ—¥å¿—", key="clear_logs"):
                            self.log_buffer.clear()
                            st.rerun()
                    with col2:
                        if st.button("ğŸ“¥ ä¸‹è½½æ—¥å¿—", key="download_logs"):
                            log_content = "\n".join(self.log_buffer)
                            st.download_button(
                                "ç¡®è®¤ä¸‹è½½",
                                log_content,
                                file_name=f"execution_logs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                                mime="text/plain"
                            )
                    with col3:
                        if st.button("ğŸ”„ åˆ·æ–°æ˜¾ç¤º", key="refresh_logs"):
                            st.rerun()
                else:
                    st.info("æš‚æ— æ—¥å¿—è¾“å‡º")
        except Exception as e:
            # å¦‚æœæ›´æ–°å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹stdout
            self.original_stdout.write(f"æ—¥å¿—æ˜¾ç¤ºæ›´æ–°å¤±è´¥: {e}\n")
    
    def get_logs(self):
        """è·å–å½“å‰æ—¥å¿—å†…å®¹"""
        return self.log_buffer.copy()


@contextmanager
def capture_logs(container):
    """ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç”¨äºæ•è·æ—¥å¿—"""
    capture = StreamlitLogCapture(container)
    sys.stdout = capture
    try:
        yield capture
    finally:
        sys.stdout = capture.original_stdout


def get_api_config():
	# ä» Streamlit Secrets è¯»å– API é…ç½®
	api_key = st.secrets.get('RESUME_API_KEY')
	base_url = st.secrets.get('RESUME_BASE_URL')
	user_id = st.secrets.get('RESUME_USER_ID')
	
	# æ£€æŸ¥æ˜¯å¦æ‰€æœ‰é…ç½®éƒ½å·²è®¾ç½®
	if not all([api_key, base_url, user_id]):
		st.error('âŒ API é…ç½®ä¸å®Œæ•´ï¼Œè¯·åœ¨ Streamlit Cloud çš„ Settings â†’ Secrets ä¸­é…ç½®ä»¥ä¸‹ä¿¡æ¯ï¼š\n'
				'- RESUME_API_KEY: API å¯†é’¥\n'
				'- RESUME_BASE_URL: API åŸºç¡€ URL\n'
				'- RESUME_USER_ID: ç”¨æˆ· ID')
		st.stop()
	
	return api_key, base_url, user_id


def strip_ext(filename: str) -> str:
	if '.' not in filename:
		return filename
	return '.'.join(filename.split('.')[:-1])


def to_excel_bytes(data: List[dict], sheet_name: str = 'ç®€å†ä¿¡æ¯') -> bytes:
	if not data:
		return b''
	df = pd.DataFrame(data)
	output = io.BytesIO()
	with pd.ExcelWriter(output, engine='openpyxl') as writer:
		df.to_excel(writer, index=False, sheet_name=sheet_name)
		ws = writer.sheets[sheet_name]
		for column in ws.columns:
			max_len = 0
			col_letter = column[0].column_letter
			for cell in column:
				try:
					val_len = len(str(cell.value)) if cell.value is not None else 0
					max_len = max(max_len, val_len)
				except Exception:
					pass
			ws.column_dimensions[col_letter].width = min(max_len + 2, 50)
	output.seek(0)
	return output.read()


def to_failed_queries_excel_bytes(failed_queries: List[dict]) -> bytes:
	if not failed_queries:
		return b''
	df = pd.DataFrame(failed_queries)
	output = io.BytesIO()
	with pd.ExcelWriter(output, engine='openpyxl') as writer:
		df.to_excel(writer, index=False, sheet_name='å¤±è´¥æŸ¥è¯¢')
	output.seek(0)
	return output.read()


def main():
	st.set_page_config(page_title='CMSR - ç®€å†ä¿¡æ¯æå–ç³»ç»Ÿ', layout='wide')
	st.title('ğŸ“‹ CMSR - ç®€å†ä¿¡æ¯æå–ç³»ç»Ÿ')
	st.caption('åœ¨äº‘ç«¯è¿è¡Œï¼Œæ— éœ€æœ¬åœ°éƒ¨ç½²ã€‚æ”¯æŒå•æ–‡ä»¶æŸ¥è¯¢ä¸æ‰¹é‡æ–‡ä»¶åç”ŸæˆæŸ¥è¯¢ã€‚')
	
	# æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€ä¿¡æ¯
	with st.sidebar:
		st.subheader("ğŸ”§ ç³»ç»ŸçŠ¶æ€")
		st.info(f"é¡µé¢åŠ è½½æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
		st.success("âœ… æ—¥å¿—ç³»ç»Ÿå·²å°±ç»ª")
		st.caption("æ—¥å¿—ç³»ç»Ÿå°†å®æ—¶æ•è·ç¨‹åºæ‰§è¡Œè¿‡ç¨‹ä¸­çš„æ‰€æœ‰printè¾“å‡º")

	# ä» Streamlit Secrets è¯»å– API é…ç½®ï¼ˆä¸æ˜¾ç¤ºåœ¨ç•Œé¢ä¸Šï¼‰
	api_key, base_url, user_id = get_api_config()

	# â€”â€”â€” æ¨¡å¼é€‰æ‹© â€”â€”â€”
	mode = st.radio('é€‰æ‹©ä¸Šä¼ æ¨¡å¼ï¼š', ['ğŸ“„ å•æ–‡ä»¶æ¨¡å¼', 'ğŸ“ æ‰¹é‡æ–‡ä»¶æ¨¡å¼'], horizontal=True)
	
	# åˆ›å»ºæ—¥å¿—æ˜¾ç¤ºåŒºåŸŸ
	log_container = st.container()
	
	# æ·»åŠ æ—¥å¿—æ˜¾ç¤ºåŒºåŸŸ
	with st.expander("ğŸ“‹ å®æ—¶æ‰§è¡Œæ—¥å¿—", expanded=False):
		st.info("æ—¥å¿—å°†åœ¨å¼€å§‹æå–æ—¶æ˜¾ç¤º")
		st.caption("ç‚¹å‡»å±•å¼€æŸ¥çœ‹è¯¦ç»†çš„æ‰§è¡Œæ—¥å¿—ä¿¡æ¯")
	
	queries: List[str] = []

	if mode == 'ğŸ“„ å•æ–‡ä»¶æ¨¡å¼':
		st.subheader('ğŸ“ ä¸Šä¼ æŸ¥è¯¢æ–‡ä»¶ï¼ˆExcel/CSV/TXTï¼‰')
		uploaded = st.file_uploader('é€‰æ‹©ä¸€ä¸ªåŒ…å«æŸ¥è¯¢åˆ—è¡¨çš„æ–‡ä»¶ï¼š', type=['xlsx', 'xls', 'csv', 'txt'])
		if uploaded is not None:
			# å°†ä¸Šä¼ æ–‡ä»¶ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ï¼Œå†å¤ç”¨ç°æœ‰ QueryLoader é€»è¾‘
			with tempfile.NamedTemporaryFile(delete=False, suffix=f".{uploaded.name.split('.')[-1]}") as tmp:
				tmp.write(uploaded.read())
				tmp_path = tmp.name
			
			loader = QueryLoader()
			queries = loader.load_queries(tmp_path)
			st.success(f'å·²è¯»å– {len(queries)} æ¡æŸ¥è¯¢')
			if queries:
				with st.expander('æŸ¥çœ‹æŸ¥è¯¢é¢„è§ˆ', expanded=False):
					st.write(pd.DataFrame({'æŸ¥è¯¢': queries}))

	else:
		st.subheader('ğŸ“ æ‰¹é‡æ–‡ä»¶åç”ŸæˆæŸ¥è¯¢')
		batch_files = st.file_uploader('é€‰æ‹©å¤šä¸ªä»»æ„ç±»å‹æ–‡ä»¶ï¼šç³»ç»Ÿä»…æå–æ–‡ä»¶å', accept_multiple_files=True)
		if batch_files:
			file_names = [bf.name for bf in batch_files]
			queries = [f"{strip_ext(name)}çš„ç®€å†æƒ…å†µ" for name in file_names]
			st.success(f'å·²ä» {len(file_names)} ä¸ªæ–‡ä»¶åç”Ÿæˆ {len(queries)} æ¡æŸ¥è¯¢')
			with st.expander('æŸ¥çœ‹ç”Ÿæˆçš„æŸ¥è¯¢', expanded=True):
				st.write(pd.DataFrame({'æ–‡ä»¶å': file_names, 'ç”Ÿæˆçš„æŸ¥è¯¢': queries}))

	# â€”â€”â€” å¼€å§‹æå– â€”â€”â€”
	st.divider()
	can_run = bool(queries)
	run = st.button('ğŸš€ å¼€å§‹æå–', disabled=not can_run)
	if run:
		# ä½¿ç”¨æ—¥å¿—æ•è·å™¨æ•è·printè¾“å‡º
		with capture_logs(log_container) as log_capture:
			with st.spinner('æ­£åœ¨æå–ç®€å†ä¿¡æ¯ï¼Œè¯·ç¨å€™...'):
				extractor = ResumeExtractor(api_key, base_url, user_id)
				data = extractor.batch_extract_resumes(queries)
			
			# æ‰§è¡Œå®Œæˆåæ›´æ–°æ—¥å¿—æ˜¾ç¤º
			log_capture.update_display()

		if not data:
			st.error('æ²¡æœ‰æˆåŠŸæå–åˆ°ä»»ä½•ç®€å†æ•°æ®')
			return

		# æ‘˜è¦ä¿¡æ¯
		summary = extractor.get_extraction_summary()
		st.success('æå–å®Œæˆï¼ä¸‹é¢æ˜¯æ‘˜è¦ä¿¡æ¯ï¼š')
		col1, col2, col3, col4 = st.columns(4)
		col1.metric('æ€»æå–æ•°é‡', summary.get('total_count', 0))
		col2.metric('æˆåŠŸæå–', summary.get('successful_extractions', 0))
		col3.metric('ä¸åŒå§“åæ•°', len(summary.get('unique_names', [])))
		col4.metric('å­¦å†ç±»å‹æ•°', len(summary.get('education_levels', [])))

		# æ•°æ®é¢„è§ˆ
		with st.expander('æŸ¥çœ‹æå–æ˜ç»†ï¼ˆå‰100è¡Œï¼‰', expanded=False):
			st.dataframe(pd.DataFrame(data).head(100), use_container_width=True)

		# ä¸‹è½½åŒº
		st.subheader('ğŸ“¥ ä¸‹è½½ç»“æœæ–‡ä»¶')
		excel_bytes = to_excel_bytes(data, sheet_name='ç®€å†ä¿¡æ¯')
		json_str = json.dumps(data, ensure_ascii=False, indent=2)
		st.download_button('ğŸ“Š ä¸‹è½½Excel', data=excel_bytes, file_name=f"resume_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx", mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')
		st.download_button('ğŸ“„ ä¸‹è½½JSON', data=json_str.encode('utf-8'), file_name=f"resume_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json", mime='application/json')

		# å¤±è´¥æŸ¥è¯¢
		failed = getattr(extractor, 'failed_queries', [])
		if failed:
			st.warning(f'æœ‰ {len(failed)} æ¡æŸ¥è¯¢å¤±è´¥ï¼Œå¯ä¸‹è½½æ˜ç»†ã€‚')
			failed_bytes = to_failed_queries_excel_bytes(failed)
			st.download_button('âš ï¸ ä¸‹è½½å¤±è´¥æŸ¥è¯¢ï¼ˆExcelï¼‰', data=failed_bytes, file_name=f"failed_queries_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx", mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')


if __name__ == '__main__':
	main()

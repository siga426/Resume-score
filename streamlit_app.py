import streamlit as st
import pandas as pd
import os
import json
import tempfile
import zipfile
from datetime import datetime
import sys
import io
from contextlib import redirect_stdout
import threading
import queue
import time

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from resume_extractor import ResumeExtractor
from query_loader import QueryLoader

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="ç®€å†ä¿¡æ¯æå–ç³»ç»Ÿ",
    page_icon="ğŸ“‹",
    layout="wide",
    initial_sidebar_state="expanded"
)

# å…¨å±€å˜é‡
if 'log_messages' not in st.session_state:
    st.session_state.log_messages = []
if 'extraction_running' not in st.session_state:
    st.session_state.extraction_running = False

class StreamlitLogger:
    """Streamlitæ—¥å¿—è®°å½•å™¨ï¼Œç”¨äºæ•è·printè¾“å‡º"""
    
    def __init__(self):
        self.log_queue = queue.Queue()
        self.original_stdout = sys.stdout
        self.original_print = print
        
    def start_capture(self):
        """å¼€å§‹æ•è·printè¾“å‡º"""
        sys.stdout = self
        # é‡å†™printå‡½æ•°
        def custom_print(*args, **kwargs):
            message = ' '.join(str(arg) for arg in args)
            timestamp = datetime.now().strftime('%H:%M:%S')
            log_entry = f"[{timestamp}] {message}"
            self.log_queue.put(log_entry)
            # åŒæ—¶è¾“å‡ºåˆ°åŸå§‹stdout
            self.original_print(*args, **kwargs)
        
        # æ›¿æ¢å…¨å±€printå‡½æ•°
        globals()['print'] = custom_print
        
    def stop_capture(self):
        """åœæ­¢æ•è·printè¾“å‡º"""
        sys.stdout = self.original_stdout
        globals()['print'] = self.original_print
        
    def write(self, text):
        """é‡å†™stdoutçš„writeæ–¹æ³•"""
        if text.strip():
            timestamp = datetime.now().strftime('%H:%M:%S')
            log_entry = f"[{timestamp}] {text.strip()}"
            self.log_queue.put(log_entry)
        self.original_stdout.write(text)
        
    def flush(self):
        """é‡å†™stdoutçš„flushæ–¹æ³•"""
        self.original_stdout.flush()
        
    def get_logs(self):
        """è·å–æ‰€æœ‰æ—¥å¿—æ¶ˆæ¯"""
        logs = []
        while not self.log_queue.empty():
            try:
                logs.append(self.log_queue.get_nowait())
            except queue.Empty:
                break
        return logs

def main():
    """ä¸»å‡½æ•°"""
    
    # é¡µé¢æ ‡é¢˜
    st.title("ğŸ“‹ ç®€å†ä¿¡æ¯æå–ç³»ç»Ÿ")
    st.markdown("æ™ºèƒ½æå–ç®€å†ä¿¡æ¯ï¼Œæ”¯æŒæ‰¹é‡å¤„ç† - Streamlitç‰ˆæœ¬")
    
    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.header("âš™ï¸ ç³»ç»Ÿé…ç½®")
        
        # APIé…ç½®
        st.subheader("APIè®¾ç½®")
        api_key = st.text_input("APIå¯†é’¥", value="d2a7gnen04uuiosfsnk0", type="password")
        base_url = st.text_input("APIåŸºç¡€URL", value="https://aiagentplatform.cmft.com")
        user_id = st.text_input("ç”¨æˆ·ID", value="Siga")
        
        # æ–‡ä»¶ä¸Šä¼ é…ç½®
        st.subheader("ğŸ“ æ–‡ä»¶ä¸Šä¼ ")
        uploaded_file = st.file_uploader(
            "é€‰æ‹©æŸ¥è¯¢æ–‡ä»¶",
            type=['xlsx', 'xls', 'csv', 'txt'],
            help="æ”¯æŒExcelã€CSVã€TXTæ ¼å¼ï¼Œç¬¬ä¸€åˆ—åŒ…å«æŸ¥è¯¢åˆ—è¡¨"
        )
        
        # æ“ä½œæŒ‰é’®
        st.subheader("ğŸš€ æ“ä½œæ§åˆ¶")
        if uploaded_file is not None:
            if st.button("å¼€å§‹æå–", type="primary", use_container_width=True):
                start_extraction(uploaded_file, api_key, base_url, user_id)
        
        # ç³»ç»ŸçŠ¶æ€
        st.subheader("ğŸ“Š ç³»ç»ŸçŠ¶æ€")
        if st.session_state.extraction_running:
            st.info("ğŸ”„ æ­£åœ¨è¿è¡Œä¸­...")
        else:
            st.success("âœ… ç³»ç»Ÿå°±ç»ª")
    
    # ä¸»å†…å®¹åŒºåŸŸ
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.header("ğŸ“‹ æ–‡ä»¶ä¿¡æ¯")
        if uploaded_file is not None:
            file_info = {
                "æ–‡ä»¶å": uploaded_file.name,
                "æ–‡ä»¶å¤§å°": f"{uploaded_file.size / 1024:.1f} KB",
                "æ–‡ä»¶ç±»å‹": uploaded_file.type,
                "ä¸Šä¼ æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            
            for key, value in file_info.items():
                st.write(f"**{key}:** {value}")
            
            # é¢„è§ˆæ–‡ä»¶å†…å®¹
            st.subheader("ğŸ“– æ–‡ä»¶å†…å®¹é¢„è§ˆ")
            try:
                if uploaded_file.name.endswith(('.xlsx', '.xls')):
                    df = pd.read_excel(uploaded_file)
                elif uploaded_file.name.endswith('.csv'):
                    df = pd.read_csv(uploaded_file)
                else:
                    # æ–‡æœ¬æ–‡ä»¶
                    content = uploaded_file.read().decode('utf-8')
                    lines = content.split('\n')[:10]  # åªæ˜¾ç¤ºå‰10è¡Œ
                    df = pd.DataFrame({'æŸ¥è¯¢å†…å®¹': lines})
                    uploaded_file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
                
                st.dataframe(df.head(10), use_container_width=True)
                if len(df) > 10:
                    st.info(f"æ˜¾ç¤ºå‰10è¡Œï¼Œå…±{len(df)}è¡Œæ•°æ®")
                    
            except Exception as e:
                st.error(f"æ–‡ä»¶é¢„è§ˆå¤±è´¥: {str(e)}")
        else:
            st.info("è¯·ä¸Šä¼ æŸ¥è¯¢æ–‡ä»¶")
    
    with col2:
        st.header("ğŸ“Š æå–ç»Ÿè®¡")
        if 'extraction_summary' in st.session_state:
            summary = st.session_state.extraction_summary
            col_a, col_b = st.columns(2)
            
            with col_a:
                st.metric("æ€»æ•°é‡", summary.get('total_count', 0))
                st.metric("æˆåŠŸ", summary.get('successful_extractions', 0))
            
            with col_b:
                st.metric("å¤±è´¥", summary.get('failed_count', 0))
                st.metric("æˆåŠŸç‡", f"{summary.get('success_rate', 0):.1f}%")
        else:
            st.info("æš‚æ— æå–æ•°æ®")
    
    # å®æ—¶æ—¥å¿—æ˜¾ç¤º
    st.header("ğŸ“ å®æ—¶æ‰§è¡Œæ—¥å¿—")
    
    # æ—¥å¿—æ§åˆ¶æŒ‰é’®
    col1, col2, col3 = st.columns([1, 1, 2])
    with col1:
        if st.button("ğŸ”„ åˆ·æ–°æ—¥å¿—", use_container_width=True):
            st.rerun()
    
    with col2:
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ—¥å¿—", use_container_width=True):
            st.session_state.log_messages = []
            st.rerun()
    
    with col3:
        st.info("æ—¥å¿—ä¼šå®æ—¶æ˜¾ç¤ºç¨‹åºæ‰§è¡Œè¿‡ç¨‹ä¸­çš„æ‰€æœ‰printè¾“å‡º")
    
    # æ—¥å¿—æ˜¾ç¤ºåŒºåŸŸ
    if st.session_state.log_messages:
        # åˆ›å»ºå¯æ»šåŠ¨çš„æ—¥å¿—å®¹å™¨
        log_text = "\n".join(st.session_state.log_messages[-100:])  # åªæ˜¾ç¤ºæœ€å100æ¡
        st.text_area("æ‰§è¡Œæ—¥å¿—", value=log_text, height=300, disabled=True)
    else:
        st.info("æš‚æ— æ—¥å¿—ä¿¡æ¯")
    
    # ç»“æœæ˜¾ç¤ºåŒºåŸŸ
    if 'extraction_results' in st.session_state:
        st.header("ğŸ“Š æå–ç»“æœ")
        
        results = st.session_state.extraction_results
        
        # æ˜¾ç¤ºæ‘˜è¦ä¿¡æ¯
        if 'summary' in results:
            summary = results['summary']
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("æˆåŠŸæå–", summary.get('successful_extractions', 0))
            
            with col2:
                st.metric("ä¸åŒå§“å", len(summary.get('unique_names', [])))
            
            with col3:
                st.metric("å­¦å†ç±»å‹", len(summary.get('education_levels', [])))
            
            with col4:
                st.metric("æ¶‰åŠé™¢æ ¡", len(summary.get('universities', [])))
        
        # æ˜¾ç¤ºæ•°æ®è¡¨æ ¼
        if 'data' in results and results['data']:
            st.subheader("ğŸ“‹ æå–æ•°æ®é¢„è§ˆ")
            df = pd.DataFrame(results['data'])
            st.dataframe(df.head(20), use_container_width=True)
            
            if len(df) > 20:
                st.info(f"æ˜¾ç¤ºå‰20è¡Œï¼Œå…±{len(df)}è¡Œæ•°æ®")
        
        # ä¸‹è½½æŒ‰é’®
        if 'files' in results:
            st.subheader("ğŸ“¥ ä¸‹è½½ç»“æœ")
            col1, col2, col3 = st.columns(3)
            
            with col1:
                if st.button("ğŸ“Š ä¸‹è½½Excel", use_container_width=True):
                    download_excel(results['files']['excel'])
            
            with col2:
                if st.button("ğŸ“„ ä¸‹è½½JSON", use_container_width=True):
                    download_json(results['files']['json'])
            
            with col3:
                if st.button("ğŸ“¦ ä¸‹è½½æ‰€æœ‰æ–‡ä»¶", use_container_width=True):
                    download_all_files()

def start_extraction(uploaded_file, api_key, base_url, user_id):
    """å¼€å§‹ç®€å†æå–"""
    
    if st.session_state.extraction_running:
        st.warning("æå–ä»»åŠ¡æ­£åœ¨è¿è¡Œä¸­ï¼Œè¯·ç­‰å¾…å®Œæˆ")
        return
    
    st.session_state.extraction_running = True
    
    # åˆ›å»ºæ—¥å¿—è®°å½•å™¨
    logger = StreamlitLogger()
    logger.start_capture()
    
    try:
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        upload_dir = "uploads"
        os.makedirs(upload_dir, exist_ok=True)
        
        file_path = os.path.join(upload_dir, uploaded_file.name)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        # è®°å½•æ—¥å¿—
        st.session_state.log_messages.append(f"[{datetime.now().strftime('%H:%M:%S')}] æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {uploaded_file.name}")
        
        # åˆ›å»ºç®€å†æå–å™¨
        extractor = ResumeExtractor(api_key, base_url, user_id)
        query_loader = QueryLoader()
        
        # ä»æ–‡ä»¶è¯»å–æŸ¥è¯¢åˆ—è¡¨
        resume_queries = query_loader.load_queries(file_path)
        
        if not resume_queries:
            st.error("æ— æ³•ä»æ–‡ä»¶ä¸­è¯»å–æŸ¥è¯¢åˆ—è¡¨")
            return
        
        st.session_state.log_messages.append(f"[{datetime.now().strftime('%H:%M:%S')}] æˆåŠŸåŠ è½½ {len(resume_queries)} ä¸ªæŸ¥è¯¢")
        
        # æ‰§è¡Œæ‰¹é‡æå–
        extracted_data = extractor.batch_extract_resumes(resume_queries)
        
        if not extracted_data:
            st.error("æ²¡æœ‰æˆåŠŸæå–åˆ°ä»»ä½•ç®€å†æ•°æ®")
            return
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        excel_filename = f'resume_data_{timestamp}.xlsx'
        json_filename = f'resume_data_{timestamp}.json'
        
        output_dir = "outputs"
        os.makedirs(output_dir, exist_ok=True)
        
        excel_path = os.path.join(output_dir, excel_filename)
        json_path = os.path.join(output_dir, json_filename)
        
        # å¯¼å‡ºæ–‡ä»¶
        extractor.export_to_excel(excel_path)
        extractor.export_to_json(json_path)
        
        # ä¿å­˜å¤±è´¥çš„æŸ¥è¯¢ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        failed_queries_file = None
        if hasattr(extractor, 'failed_queries') and extractor.failed_queries:
            failed_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            failed_filename = f'failed_queries_{failed_timestamp}.xlsx'
            failed_path = os.path.join(output_dir, failed_filename)
            extractor.save_failed_queries(failed_path)
            failed_queries_file = failed_filename
        
        # è·å–æå–æ‘˜è¦
        summary = extractor.get_extraction_summary()
        failed_summary = extractor.get_failed_queries_summary()
        
        # ä¿å­˜ç»“æœåˆ°session state
        st.session_state.extraction_results = {
            'data': extracted_data,
            'summary': summary,
            'failed_summary': failed_summary,
            'files': {
                'excel': excel_filename,
                'json': json_filename,
                'failed_queries': failed_queries_file
            }
        }
        
        st.session_state.extraction_summary = {
            'total_count': len(resume_queries),
            'successful_extractions': summary.get('successful_extractions', 0),
            'failed_count': failed_summary.get('failed_count', 0) if failed_summary else 0,
            'success_rate': (summary.get('successful_extractions', 0) / len(resume_queries)) * 100 if resume_queries else 0
        }
        
        st.session_state.log_messages.append(f"[{datetime.now().strftime('%H:%M:%S')}] ç®€å†æå–å®Œæˆï¼æˆåŠŸæå– {len(extracted_data)} æ¡æ•°æ®")
        
        st.success("ç®€å†æå–å®Œæˆï¼")
        
    except Exception as e:
        error_msg = f"æå–å¤±è´¥: {str(e)}"
        st.error(error_msg)
        st.session_state.log_messages.append(f"[{datetime.now().strftime('%H:%M:%S')}] {error_msg}")
        
    finally:
        # åœæ­¢æ—¥å¿—æ•è·
        logger.stop_capture()
        st.session_state.extraction_running = False
        
        # è·å–å‰©ä½™çš„æ—¥å¿—
        remaining_logs = logger.get_logs()
        st.session_state.log_messages.extend(remaining_logs)

def download_excel(filename):
    """ä¸‹è½½Excelæ–‡ä»¶"""
    try:
        file_path = os.path.join("outputs", filename)
        if os.path.exists(file_path):
            with open(file_path, "rb") as f:
                st.download_button(
                    label="ç‚¹å‡»ä¸‹è½½Excelæ–‡ä»¶",
                    data=f.read(),
                    file_name=filename,
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
        else:
            st.error("æ–‡ä»¶ä¸å­˜åœ¨")
    except Exception as e:
        st.error(f"ä¸‹è½½å¤±è´¥: {str(e)}")

def download_json(filename):
    """ä¸‹è½½JSONæ–‡ä»¶"""
    try:
        file_path = os.path.join("outputs", filename)
        if os.path.exists(file_path):
            with open(file_path, "rb") as f:
                st.download_button(
                    label="ç‚¹å‡»ä¸‹è½½JSONæ–‡ä»¶",
                    data=f.read(),
                    file_name=filename,
                    mime="application/json"
                )
        else:
            st.error("æ–‡ä»¶ä¸å­˜åœ¨")
    except Exception as e:
        st.error(f"ä¸‹è½½å¤±è´¥: {str(e)}")

def download_all_files():
    """ä¸‹è½½æ‰€æœ‰è¾“å‡ºæ–‡ä»¶"""
    try:
        output_dir = "outputs"
        if not os.path.exists(output_dir):
            st.error("è¾“å‡ºç›®å½•ä¸å­˜åœ¨")
            return
        
        # åˆ›å»ºZIPæ–‡ä»¶
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        zip_filename = f'resume_extraction_{timestamp}.zip'
        zip_path = os.path.join(output_dir, zip_filename)
        
        with zipfile.ZipFile(zip_path, 'w') as zipf:
            for filename in os.listdir(output_dir):
                if filename.endswith(('.xlsx', '.json')):
                    filepath = os.path.join(output_dir, filename)
                    zipf.write(filepath, filename)
        
        # æä¾›ä¸‹è½½
        with open(zip_path, "rb") as f:
            st.download_button(
                label="ç‚¹å‡»ä¸‹è½½æ‰€æœ‰æ–‡ä»¶(ZIP)",
                data=f.read(),
                file_name=zip_filename,
                mime="application/zip"
            )
        
        # åˆ é™¤ä¸´æ—¶ZIPæ–‡ä»¶
        os.remove(zip_path)
        
    except Exception as e:
        st.error(f"æ‰“åŒ…ä¸‹è½½å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    main()

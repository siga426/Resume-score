import os
import io
import json
import zipfile
import tempfile
import contextlib
from datetime import datetime
from typing import List, Tuple

import streamlit as st
import pandas as pd

from resume_extractor import ResumeExtractor
from resume_scorer import ResumeScorer
from query_loader import QueryLoader

# ç¡¬ç¼–ç APIé…ç½®ï¼ˆæŒ‰ä½ çš„è¦æ±‚ï¼‰
EXTRACT_API_KEY = 'd2a7gnen04uuiosfsnk0'
SCORE_API_KEY_HARDCODED = 'd2ji4jh6ht5pktrvmql0'
BASE_URL = 'https://aiagentplatform.cmft.com'
USER_ID = 'Siga'

# è¿è¡Œæ—¶æ£€æŸ¥ç¬¬ä¸‰æ–¹å¹³å°SDKæ˜¯å¦å¯ç”¨ï¼Œç»™å‡ºæ›´å‹å¥½çš„æç¤º
try:
    import aiagentplatformpy  # type: ignore
    _HAS_AIA = True
except Exception:
    _HAS_AIA = False


def get_api_config_from_secrets() -> Tuple[str, str, str]:
	# æ”¹ä¸ºè¿”å›ç¡¬ç¼–ç é…ç½®ï¼Œä¸å†ä» Secrets è¯»å–
	return EXTRACT_API_KEY, BASE_URL, USER_ID


def get_score_key_from_secrets() -> str:
	# æ”¹ä¸ºè¿”å›ç¡¬ç¼–ç çš„è¯„åˆ† Keyï¼Œä¸å†ä» Secrets è¯»å–
	return SCORE_API_KEY_HARDCODED


def strip_ext(filename: str) -> str:
	if '.' not in filename:
		return filename
	return '.'.join(filename.split('.')[:-1])


def to_excel_bytes(data: List[dict], sheet_name: str = 'ç®€å†ä¿¡æ¯') -> bytes:
	if not data:
		return b''
	df = pd.DataFrame(data)
	output = io.BytesIO()
	with pd.ExcelWriter(output, engine='openpyxl') as writer:
		df.to_excel(writer, index=False, sheet_name=sheet_name)
	output.seek(0)
	return output.read()


def to_failed_queries_excel_bytes(failed_queries: List[dict]) -> bytes:
	if not failed_queries:
		return b''
	df = pd.DataFrame(failed_queries)
	output = io.BytesIO()
	with pd.ExcelWriter(output, engine='openpyxl') as writer:
		df.to_excel(writer, index=False, sheet_name='å¤±è´¥æŸ¥è¯¢')
	output.seek(0)
	return output.read()


def build_zip_bytes(files: List[Tuple[str, bytes]]) -> bytes:
	buf = io.BytesIO()
	with zipfile.ZipFile(buf, 'w', zipfile.ZIP_DEFLATED) as zf:
		for name, data in files:
			zf.writestr(name, data)
	buf.seek(0)
	return buf.read()


def main():
	st.set_page_config(page_title='CMSR - ç®€å†ä¿¡æ¯æå–ç³»ç»Ÿ', layout='wide')
	st.title('ğŸ“‹ CMSR - ç®€å†ä¿¡æ¯æå–ç³»ç»Ÿ')

	# â€”â€”â€” ä¸»ç•Œé¢é…ç½®ä¿¡æ¯ â€”â€”â€”
	st.subheader('âš™ï¸ è¿è¡Œé…ç½®ï¼ˆä»£ç å†…å†™æ­»ï¼‰')
	api_key, base_url, user_id = get_api_config_from_secrets()
	score_api_key_input = get_score_key_from_secrets()
	st.caption('æœ¬åº”ç”¨ä¸ä½¿ç”¨ Secretsã€‚')

	if not _HAS_AIA:
		st.warning('æœªæ£€æµ‹åˆ° aiagentplatformpyã€‚è‹¥ä¸ºç§æœ‰åº“ï¼Œäº‘ç«¯æ— æ³•ç›´æ¥å®‰è£…ï¼Œè¯·ä½¿ç”¨å¸¦è¯¥åº“çš„è‡ªå®šä¹‰ç¯å¢ƒæˆ–ç§æœ‰åŒ…é•œåƒï¼›æˆ–è”ç³»ç®¡ç†å‘˜æä¾›å…¬å…±å¯å®‰è£…ç‰ˆæœ¬ã€‚')

	st.markdown('---')
	st.subheader('ğŸ“Œ ä½¿ç”¨æç¤º')
	st.markdown('- æ”¯æŒ Excel/CSV/TXT ä¸‰ç§è¾“å…¥æ–¹å¼')
	st.markdown('- ä¹Ÿå¯ä»æ–‡ä»¶åå¿«é€Ÿç”ŸæˆæŸ¥è¯¢æˆ–æ‰‹åŠ¨ç²˜è´´æŸ¥è¯¢')
	st.markdown('- å¤„ç†å®Œæˆåå¯ä¸‹è½½åˆå¹¶Excelï¼ˆä¿¡æ¯+è¯„åˆ†ï¼‰ã€å•ç‹¬Excelã€JSONã€å¤±è´¥æŸ¥è¯¢ä¸ZIP')

	# â€”â€”â€” æ¨¡å¼é€‰æ‹© â€”â€”â€”
	mode = st.radio('é€‰æ‹©æ¨¡å¼ï¼š', ['ğŸ“„ å•æ–‡ä»¶ä¸Šä¼ ', 'ğŸ“ ä»æ–‡ä»¶åç”Ÿæˆ', 'ğŸ“ æ‰‹åŠ¨æ‰¹é‡è¾“å…¥'], horizontal=True)

	queries: List[str] = []

	if mode == 'ğŸ“„ å•æ–‡ä»¶ä¸Šä¼ ':
		st.subheader('ğŸ“ ä¸Šä¼ æŸ¥è¯¢æ–‡ä»¶ï¼ˆExcel/CSV/TXTï¼‰')
		uploaded = st.file_uploader('é€‰æ‹©ä¸€ä¸ªåŒ…å«æŸ¥è¯¢åˆ—è¡¨çš„æ–‡ä»¶ï¼š', type=['xlsx', 'xls', 'csv', 'txt'])
		if uploaded is not None:
			with tempfile.NamedTemporaryFile(delete=False, suffix=f".{uploaded.name.split('.')[-1]}") as tmp:
				tmp.write(uploaded.read())
				tmp_path = tmp.name
			loader = QueryLoader()
			queries = loader.load_queries(tmp_path)
			st.success(f'å·²è¯»å– {len(queries)} æ¡æŸ¥è¯¢')
			if queries:
				with st.expander('æŸ¥çœ‹æŸ¥è¯¢é¢„è§ˆ', expanded=False):
					st.write(pd.DataFrame({'æŸ¥è¯¢': queries}))

	elif mode == 'ğŸ“ ä»æ–‡ä»¶åç”Ÿæˆ':
		st.subheader('ğŸ“ é€‰æ‹©å¤šä¸ªæ–‡ä»¶ï¼Œç³»ç»Ÿå°†åŸºäºæ–‡ä»¶åç”ŸæˆæŸ¥è¯¢')
		batch_files = st.file_uploader('é€‰æ‹©å¤šä¸ªä»»æ„ç±»å‹æ–‡ä»¶ï¼šä»…æå–æ–‡ä»¶å', accept_multiple_files=True)
		if batch_files:
			file_names = [bf.name for bf in batch_files]
			extract_queries = [f"{strip_ext(name)}çš„ç®€å†æƒ…å†µ" for name in file_names]
			score_queries = [f"{strip_ext(name)}çš„ç®€å†è¯„åˆ†" for name in file_names]
			queries = extract_queries  # ç”¨äºæå–çš„æŸ¥è¯¢
			st.success(f'å·²ä» {len(file_names)} ä¸ªæ–‡ä»¶åç”Ÿæˆ {len(queries)} æ¡æŸ¥è¯¢')
			with st.expander('æŸ¥çœ‹ç”Ÿæˆçš„æŸ¥è¯¢', expanded=True):
				st.write(pd.DataFrame({
					'æ–‡ä»¶å': file_names, 
					'æå–æŸ¥è¯¢': extract_queries,
					'è¯„åˆ†æŸ¥è¯¢': score_queries
				}))

	else:
		st.subheader('ğŸ“ æ‰‹åŠ¨ç²˜è´´æ‰¹é‡æŸ¥è¯¢ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰')
		text = st.text_area('åœ¨æ­¤ç²˜è´´æˆ–è¾“å…¥ï¼Œæ¯è¡Œä¸€ä¸ªæŸ¥è¯¢ï¼ˆè‡ªåŠ¨è¡¥é½â€œçš„ç®€å†æƒ…å†µ/ç®€å†ä¿¡æ¯â€åç¼€ï¼‰', height=200)
		if text.strip():
			raw = [ln.strip() for ln in text.split('\n') if ln.strip()]
			queries = []
			for q in raw:
				if q.endswith('çš„ç®€å†ä¿¡æ¯') or q.endswith('çš„ç®€å†æƒ…å†µ'):
					queries.append(q)
				else:
					queries.append(q + 'çš„ç®€å†ä¿¡æ¯')
			st.success(f'å…±æ”¶é›† {len(queries)} æ¡æŸ¥è¯¢')
			with st.expander('æŸ¥è¯¢åˆ—è¡¨', expanded=False):
				st.write(pd.DataFrame({'æŸ¥è¯¢': queries}))
			# ç”Ÿæˆå¯ä¸‹è½½çš„TXT
			if queries:
				txt_buf = io.StringIO('\n'.join(queries))
				st.download_button('ğŸ“ ä¸‹è½½æŸ¥è¯¢TXT', data=txt_buf.getvalue().encode('utf-8'), file_name=f"batch_queries_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt", mime='text/plain')

	st.divider()
	can_run = bool(queries)
	run = st.button('ğŸš€ å¼€å§‹æå–ä¸è¯„åˆ†', disabled=not can_run)

	# ä½¿ç”¨ session_state ä¿å­˜é˜¶æ®µæ€§ç»“æœ
	if 'extracted_results' not in st.session_state:
		st.session_state.extracted_results = None
	if 'extracted_failed' not in st.session_state:
		st.session_state.extracted_failed = None
	if 'score_results' not in st.session_state:
		st.session_state.score_results = None
	if 'score_error' not in st.session_state:
		st.session_state.score_error = None
	# åˆå§‹åŒ–æ—¥å¿—å­˜å‚¨å¹¶å¸¸é©»æ˜¾ç¤ºæ—¥å¿—åŒºåŸŸï¼ˆè¿è¡Œç»“æŸåä»å¯è§ï¼‰
	if 'extract_logs' not in st.session_state:
		st.session_state.extract_logs = []
	if 'score_logs' not in st.session_state:
		st.session_state.score_logs = []

	# å¸¸é©»æ—¥å¿—åŒºåŸŸï¼ˆé»˜è®¤å±•å¼€ï¼Œæ˜¾ç¤ºå½“å‰ session_state æ—¥å¿—ï¼‰
	# æå–è¿›åº¦æ¡
	ex_progress_placeholder = st.empty()
	
	ex_log_expander = st.expander('ğŸ“œ æå–æ—¥å¿—', expanded=True)
	with ex_log_expander:
		# æ˜¾ç¤ºå·²æœ‰çš„æå–æ—¥å¿—
		if st.session_state.extract_logs:
			st.text_area(
				label='æå–æ—¥å¿—å†…å®¹',
				value=''.join(st.session_state.extract_logs),
				height=200,
				disabled=True,
				key='extract_log_display'
			)
		ex_log_placeholder = st.empty()

	# è¯„åˆ†è¿›åº¦æ¡
	sc_progress_placeholder = st.empty()
	
	sc_expander = st.expander('ğŸ“œ è¯„åˆ†æ—¥å¿—', expanded=True)
	with sc_expander:
		# æ˜¾ç¤ºå·²æœ‰çš„è¯„åˆ†æ—¥å¿—
		if st.session_state.score_logs:
			st.text_area(
				label='è¯„åˆ†æ—¥å¿—å†…å®¹',
				value=''.join(st.session_state.score_logs),
				height=200,
				disabled=True,
				key='score_log_display'
			)
		sc_placeholder = st.empty()

	# æå–æµç¨‹ä¸è¯„åˆ†æµç¨‹ï¼ˆåˆå¹¶æŒ‰é’®é¡ºåºæ‰§è¡Œï¼‰
	if run:
		with ex_progress_placeholder:
			progress_ex = st.progress(0, text='æå–å¼€å§‹...')
		# åˆå§‹åŒ–/æ¸…ç©ºæå–æ—¥å¿—
		st.session_state['extract_logs'] = []

		class StreamlitAppendWriter(io.StringIO):
			def write(self, s: str):
				if not s:
					return
				st.session_state['extract_logs'].append(s)
				# æ›´æ–°å›ºå®šçš„æ—¥å¿—æ˜¾ç¤ºåŒºåŸŸ
				ex_log_placeholder.text_area(
					label='å®æ—¶æå–æ—¥å¿—',
					value=''.join(st.session_state['extract_logs']),
					height=200,
					disabled=True,
					key=f'extract_log_realtime_{len(st.session_state["extract_logs"])}'  # åŠ¨æ€key
				)

		extractor = ResumeExtractor(api_key, base_url, user_id)
		# å¤ç”¨æå–ä¼šè¯ID
		if st.session_state.get('extract_conversation_id'):
			extractor.chat_api.conversation_id = st.session_state['extract_conversation_id']
		else:
			conv_id = extractor.chat_api.create_or_load_conversation(use_existing=True)
			st.session_state['extract_conversation_id'] = conv_id
		results = []
		failed = []
		with contextlib.redirect_stdout(StreamlitAppendWriter()):
			for idx, q in enumerate(queries, 1):
				print(f"\n=== å¤„ç†ç¬¬{idx}ä¸ªç®€å†æŸ¥è¯¢ ===")
				print(f"æŸ¥è¯¢: {q}")
				info = extractor.process_resume_query(q)
				if info:
					print("âœ… æˆåŠŸæå–ç®€å†ä¿¡æ¯")
					results.append(info)
				else:
					print("âŒ æå–ç®€å†ä¿¡æ¯å¤±è´¥")
					failed.append({
						'åºå·': idx,
						'æŸ¥è¯¢å†…å®¹': q,
						'å¤±è´¥æ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
						'å¤±è´¥åŸå› ': 'æå–å¤±è´¥æˆ–æ— è¿”å›æ•°æ®æˆ–æ‰€æœ‰å­—æ®µä¸ºç©º'
					})
				progress_ex.progress(int(idx * 100 / len(queries)), text=f'æå–è¿›åº¦ï¼š{idx}/{len(queries)}')
		st.session_state.extracted_results = results
		st.session_state.extracted_failed = failed

	# è¯„åˆ†æµç¨‹
	if run:
		with sc_progress_placeholder:
			progress_sc = st.progress(0, text='è¯„åˆ†å¼€å§‹...')
		# åˆå§‹åŒ–/æ¸…ç©ºè¯„åˆ†æ—¥å¿—
		st.session_state['score_logs'] = []

		class StreamlitScoreWriter(io.StringIO):
			def write(self, s: str):
				if not s:
					return
				st.session_state['score_logs'].append(s)
				# æ›´æ–°å›ºå®šçš„æ—¥å¿—æ˜¾ç¤ºåŒºåŸŸ
				sc_placeholder.text_area(
					label='å®æ—¶è¯„åˆ†æ—¥å¿—',
					value=''.join(st.session_state['score_logs']),
					height=200,
					disabled=True,
					key=f'score_log_realtime_{len(st.session_state["score_logs"])}'  # åŠ¨æ€key
				)

		# ä»…ä½¿ç”¨è¯„åˆ†Keyï¼Œä¸ä½¿ç”¨å…œåº•
		use_key = score_api_key_input
		scorer = ResumeScorer(use_key, base_url, user_id)
		# å¤ç”¨è¯„åˆ†ä¼šè¯ID
		if st.session_state.get('score_conversation_id'):
			scorer.chat_api.conversation_id = st.session_state['score_conversation_id']
		else:
			conv_id = scorer.chat_api.create_or_load_conversation(use_existing=True)
			st.session_state['score_conversation_id'] = conv_id
		def to_score_query(q: str) -> str:
			base = str(q).strip()
			for suf in ['çš„ç®€å†ä¿¡æ¯', 'çš„ç®€å†æƒ…å†µ', 'çš„ç®€å†']:
				if base.endswith(suf):
					base = base[:-len(suf)]
					break
			return base + 'çš„ç®€å†è¯„åˆ†'
		score_queries = [to_score_query(q) for q in queries]
		score_data = []
		score_error = None
		with contextlib.redirect_stdout(StreamlitScoreWriter()):
			for idx, q in enumerate(score_queries, 1):
				print(f"\n=== å¤„ç†ç¬¬{idx}ä¸ªè¯„åˆ†æŸ¥è¯¢ ===")
				print(f"æŸ¥è¯¢: {q}")
				try:
					info = scorer.process_score_query(q)
					if info:
						print("âœ… æˆåŠŸè·å–è¯„åˆ†")
					else:
						print("âŒ è¯„åˆ†è¿”å›ç©ºæ•°æ®")
				except Exception as e:
					info = None
					score_error = f'è¯„åˆ†è°ƒç”¨å¤±è´¥: {e}'
					print(f"âŒ è¯„åˆ†å¼‚å¸¸: {e}")
				if info is None and score_error is None:
					score_error = 'è¯„åˆ†è°ƒç”¨å¤±è´¥æˆ–æ— è¿”å›æ•°æ®'
				if info:
					score_data.append(info)
				progress_sc.progress(int(idx * 100 / len(score_queries)), text=f'è¯„åˆ†è¿›åº¦ï¼š{idx}/{len(score_queries)}')
		st.session_state.score_results = score_data
		st.session_state.score_error = score_error

	# å±•ç¤ºæå–ç»“æœ
	if st.session_state.extracted_results is not None:
		results = st.session_state.extracted_results
		failed = st.session_state.extracted_failed or []
		if not results:
			st.error('æ²¡æœ‰æˆåŠŸæå–åˆ°ä»»ä½•ç®€å†æ•°æ®')
		else:
			extractor_tmp = ResumeExtractor(api_key, base_url, user_id)
			extractor_tmp.extracted_data = results
			meta = extractor_tmp.get_extraction_summary()
			st.success('æå–å®Œæˆï¼')
			col1, col2, col3, col4 = st.columns(4)
			col1.metric('æ€»æå–æ•°é‡', meta.get('total_count', 0))
			col2.metric('æˆåŠŸæå–', meta.get('successful_extractions', 0))
			col3.metric('ä¸åŒå§“åæ•°', len(meta.get('unique_names', [])))
			col4.metric('å­¦å†ç±»å‹æ•°', len(meta.get('education_levels', [])))
			with st.expander('æŸ¥çœ‹æå–æ˜ç»†ï¼ˆå‰100è¡Œï¼‰', expanded=False):
				st.dataframe(pd.DataFrame(results).head(100), use_container_width=True)
			# ä¸‹è½½æå–ç»“æœ
			st.subheader('ğŸ“¥ ä¸‹è½½æå–ç»“æœ')
			ts = datetime.now().strftime('%Y%m%d_%H%M%S')
			excel_bytes = to_excel_bytes(results, sheet_name='ç®€å†ä¿¡æ¯')
			st.download_button('ğŸ“Š ä¸‹è½½ç®€å†Excel', data=excel_bytes, file_name=f"resume_data_{ts}.xlsx", mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')
			if failed:
				failed_bytes = to_failed_queries_excel_bytes(failed)
				st.download_button('âš ï¸ ä¸‹è½½å¤±è´¥æŸ¥è¯¢ï¼ˆExcelï¼‰', data=failed_bytes, file_name=f"failed_queries_{ts}.xlsx", mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')

	# å±•ç¤ºè¯„åˆ†ç»“æœ
	if st.session_state.score_results is not None:
		score_data = st.session_state.score_results
		score_error = st.session_state.score_error
		if score_error:
			st.info(f'è¯„åˆ†æç¤ºï¼š{score_error}')
		if score_data:
			# æŒ‰æ€»å¾—åˆ†ä»é«˜åˆ°ä½æ’åº
			df_scores = pd.DataFrame(score_data)
			if 'æ€»å¾—åˆ†' in df_scores.columns:
				df_scores_sorted = df_scores.sort_values('æ€»å¾—åˆ†', ascending=False)
				st.success(f'è¯„åˆ†å®Œæˆï¼å…± {len(score_data)} æ¡è¯„åˆ†æ•°æ®')
				
				# æå–æ–‡ä»¶åå¹¶é‡æ–°æ’åˆ—åˆ—é¡ºåº
				if 'è¯„åˆ†æŸ¥è¯¢' in df_scores_sorted.columns:
					# ä»è¯„åˆ†æŸ¥è¯¢ä¸­æå–æ–‡ä»¶å
					def extract_filename(query):
						if pd.isna(query) or not query:
							return ''
						# ç§»é™¤"çš„ç®€å†è¯„åˆ†"åç¼€
						query_str = str(query).strip()
						if query_str.endswith('çš„ç®€å†è¯„åˆ†'):
							return query_str[:-5]  # ç§»é™¤"çš„ç®€å†è¯„åˆ†"
						return query_str
					
					df_scores_sorted['å§“å'] = df_scores_sorted['è¯„åˆ†æŸ¥è¯¢'].apply(extract_filename)
				
				# é‡æ–°æ’åˆ—åˆ—é¡ºåºï¼šæ–‡ä»¶åã€æ€»åˆ†ã€å…¶ä»–å¾—åˆ†é¡¹
				score_columns = list(df_scores_sorted.columns)
				ordered_columns = []
				
				# ç¬¬ä¸€åˆ—ï¼šæ–‡ä»¶å
				if 'å§“å' in score_columns:
					ordered_columns.append('å§“å')
				
				# ç¬¬äºŒåˆ—ï¼šæ€»åˆ†
				if 'æ€»å¾—åˆ†' in score_columns:
					ordered_columns.append('æ€»å¾—åˆ†')
				
				# å…¶ä»–å¾—åˆ†åˆ—ï¼ˆæŒ‰é¡ºåºï¼‰
				score_fields = [
					'æœ¬ç§‘é™¢æ ¡åˆ†', 'ç¡•å£«é™¢æ ¡åˆ†', 'æœ¬ç§‘ä¸“ä¸šç¬¦åˆåº¦åˆ†', 'ç¡•å£«ä¸“ä¸šç¬¦åˆåº¦åˆ†', 
					'äº¤å‰å­¦ç§‘åˆ†', 'å­¦ä¹ æˆç»©åˆ†', 'è‹±è¯­æ°´å¹³åˆ†', 'ç¼–ç¨‹æŠ€èƒ½åˆ†', 
					'é¡¹ç›®å®ä¹ ç»å†åˆ†', 'å­¦ç”Ÿå·¥ä½œåˆ†', 'æŒæ¡CADç±»è½¯ä»¶åŠ åˆ†', 'AVEVA Marineè½¯ä»¶åŠ åˆ†'
				]
				for field in score_fields:
					if field in score_columns:
						ordered_columns.append(field)
				
				# æ·»åŠ å‰©ä½™åˆ—
				for col in score_columns:
					if col not in ordered_columns:
						ordered_columns.append(col)
				
				df_scores_sorted = df_scores_sorted[ordered_columns]
				
				# æ˜¾ç¤ºæ’åºåçš„è¯„åˆ†æ˜ç»†
				with st.expander('æŸ¥çœ‹è¯„åˆ†æ˜ç»†ï¼ˆæŒ‰æ€»å¾—åˆ†ä»é«˜åˆ°ä½æ’åºï¼Œå‰100è¡Œï¼‰', expanded=False):
					st.dataframe(df_scores_sorted.head(100), use_container_width=True)
				
				# æ˜¾ç¤ºè¯„åˆ†ç»Ÿè®¡ä¿¡æ¯
				col1, col2, col3, col4 = st.columns(4)
				col1.metric('æœ€é«˜åˆ†', df_scores_sorted['æ€»å¾—åˆ†'].max())
				col2.metric('æœ€ä½åˆ†', df_scores_sorted['æ€»å¾—åˆ†'].min())
				col3.metric('å¹³å‡åˆ†', f"{df_scores_sorted['æ€»å¾—åˆ†'].mean():.1f}")
				col4.metric('ä¸­ä½æ•°', f"{df_scores_sorted['æ€»å¾—åˆ†'].median():.1f}")
			else:
				# å¦‚æœæ²¡æœ‰æ€»å¾—åˆ†å­—æ®µï¼ŒæŒ‰åŸæ ·æ˜¾ç¤º
				with st.expander('æŸ¥çœ‹è¯„åˆ†æ˜ç»†ï¼ˆå‰100è¡Œï¼‰', expanded=False):
					st.dataframe(pd.DataFrame(score_data).head(100), use_container_width=True)
			# ä¸‹è½½è¯„åˆ†ç»“æœ
			st.subheader('ğŸ“¥ ä¸‹è½½è¯„åˆ†ç»“æœ')
			ts = datetime.now().strftime('%Y%m%d_%H%M%S')
			# è‹¥æœ‰æå–ç»“æœï¼Œåˆ™æä¾›åˆå¹¶Excelä¸ZIP
			if st.session_state.extracted_results:
				# å°†è¯„åˆ†æ•°æ®æ‹¼æ¥åˆ°ç®€å†ä¿¡æ¯å³ä¾§
				df_resume = pd.DataFrame(st.session_state.extracted_results)
				df_score = pd.DataFrame(score_data)
				
				# ç”±äºæŸ¥è¯¢æ–‡ä»¶åé¡ºåºä¸€è‡´ï¼Œç›´æ¥æŒ‰ç´¢å¼•åˆå¹¶ï¼ˆæ›´å¯é ï¼‰
				merged_df = pd.concat([df_resume, df_score], axis=1)
				
				# ç”Ÿæˆåˆå¹¶åçš„Excel
				combined_output = io.BytesIO()
				with pd.ExcelWriter(combined_output, engine='openpyxl') as writer:
					merged_df.to_excel(writer, index=False, sheet_name='ç®€å†ä¿¡æ¯ä¸è¯„åˆ†')
				combined_output.seek(0)
				st.download_button('ğŸ“’ ä¸‹è½½åˆå¹¶Excelï¼ˆä¿¡æ¯+è¯„åˆ†ï¼‰', data=combined_output.read(), file_name=f"resume_with_scores_{ts}.xlsx", mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')
				# ç”Ÿæˆè¯„åˆ†JSONæ•°æ®ç”¨äºZIPåŒ…
				scores_json_bytes = json.dumps(score_data, ensure_ascii=False, indent=2).encode('utf-8')
				files_for_zip: List[Tuple[str, bytes]] = [
					(f'resume_with_scores_{ts}.xlsx', combined_output.getvalue()),
					(f'resume_data_{ts}.xlsx', to_excel_bytes(st.session_state.extracted_results)),
					(f'resume_data_{ts}.json', json.dumps(st.session_state.extracted_results, ensure_ascii=False, indent=2).encode('utf-8')),
					(f'resume_scores_{ts}.json', scores_json_bytes)
				]
				if st.session_state.extracted_failed:
					files_for_zip.append((f'failed_queries_{ts}.xlsx', to_failed_queries_excel_bytes(st.session_state.extracted_failed)))
				zip_bytes = build_zip_bytes(files_for_zip)
				st.download_button('ğŸ—œï¸ ä¸‹è½½å…¨éƒ¨ï¼ˆZIPï¼‰', data=zip_bytes, file_name=f"resume_extraction_{ts}.zip", mime='application/zip')


if __name__ == '__main__':
	main()

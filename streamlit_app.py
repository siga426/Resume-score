import os
import io
import json
import zipfile
import tempfile
import contextlib
from datetime import datetime
from typing import List, Tuple

import streamlit as st
import pandas as pd

from resume_extractor import ResumeExtractor
from resume_scorer import ResumeScorer
from query_loader import QueryLoader

# ç¡¬ç¼–ç APIé…ç½®ï¼ˆæŒ‰ä½ çš„è¦æ±‚ï¼‰
EXTRACT_API_KEY = 'd2a7gnen04uuiosfsnk0'
SCORE_API_KEY_HARDCODED = 'd2ji4jh6ht5pktrvmql0'
BASE_URL = 'https://aiagentplatform.cmft.com'
USER_ID = 'Siga'

# è¿è¡Œæ—¶æ£€æŸ¥ç¬¬ä¸‰æ–¹å¹³å°SDKæ˜¯å¦å¯ç”¨ï¼Œç»™å‡ºæ›´å‹å¥½çš„æç¤º
try:
    import aiagentplatformpy  # type: ignore
    _HAS_AIA = True
except Exception:
    _HAS_AIA = False


def get_api_config_from_secrets() -> Tuple[str, str, str]:
	# æ”¹ä¸ºè¿”å›ç¡¬ç¼–ç é…ç½®ï¼Œä¸å†ä» Secrets è¯»å–
	return EXTRACT_API_KEY, BASE_URL, USER_ID


def get_score_key_from_secrets() -> str:
	# æ”¹ä¸ºè¿”å›ç¡¬ç¼–ç çš„è¯„åˆ† Keyï¼Œä¸å†ä» Secrets è¯»å–
	return SCORE_API_KEY_HARDCODED


def strip_ext(filename: str) -> str:
	if '.' not in filename:
		return filename
	return '.'.join(filename.split('.')[:-1])


def to_excel_bytes(data: List[dict], sheet_name: str = 'ç®€å†ä¿¡æ¯') -> bytes:
	if not data:
		return b''
	df = pd.DataFrame(data)
	output = io.BytesIO()
	with pd.ExcelWriter(output, engine='openpyxl') as writer:
		df.to_excel(writer, index=False, sheet_name=sheet_name)
	output.seek(0)
	return output.read()


def to_failed_queries_excel_bytes(failed_queries: List[dict]) -> bytes:
	if not failed_queries:
		return b''
	df = pd.DataFrame(failed_queries)
	output = io.BytesIO()
	with pd.ExcelWriter(output, engine='openpyxl') as writer:
		df.to_excel(writer, index=False, sheet_name='å¤±è´¥æŸ¥è¯¢')
	output.seek(0)
	return output.read()


def build_zip_bytes(files: List[Tuple[str, bytes]]) -> bytes:
	buf = io.BytesIO()
	with zipfile.ZipFile(buf, 'w', zipfile.ZIP_DEFLATED) as zf:
		for name, data in files:
			zf.writestr(name, data)
	buf.seek(0)
	return buf.read()


def main():
	st.set_page_config(page_title='CMSR - ç®€å†ä¿¡æ¯æå–ç³»ç»Ÿ', layout='wide')
	
	# è‡ªå®šä¹‰CSSæ ·å¼
	st.markdown("""
	<style>
	/* ä¸»æ ‡é¢˜æ ·å¼ */
	.main-header {
		background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
		padding: 2rem;
		border-radius: 15px;
		margin-bottom: 2rem;
		text-align: center;
		color: white;
		box-shadow: 0 8px 32px rgba(0,0,0,0.1);
	}
	
	/* å¡ç‰‡æ ·å¼ */
	.stCard {
		background: white;
		padding: 1.5rem;
		border-radius: 10px;
		box-shadow: 0 4px 16px rgba(0,0,0,0.1);
		margin: 1rem 0;
		border-left: 4px solid #667eea;
	}
	
	/* æŒ‰é’®æ ·å¼ */
	.stButton > button {
		background: linear-gradient(45deg, #667eea, #764ba2);
		color: white;
		border: none;
		border-radius: 25px;
		padding: 0.75rem 2rem;
		font-weight: 600;
		transition: all 0.3s ease;
		box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4);
	}
	
	.stButton > button:hover {
		transform: translateY(-2px);
		box-shadow: 0 6px 20px rgba(102, 126, 234, 0.6);
	}
	
	/* æŒ‡æ ‡å¡ç‰‡æ ·å¼ */
	.metric-card {
		background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
		color: white;
		padding: 1.5rem;
		border-radius: 15px;
		text-align: center;
		box-shadow: 0 8px 25px rgba(240, 147, 251, 0.3);
	}
	
	/* æˆåŠŸæç¤ºæ ·å¼ */
	.success-box {
		background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
		color: white;
		padding: 1rem;
		border-radius: 10px;
		margin: 1rem 0;
		text-align: center;
		font-weight: 600;
	}
	
	/* è­¦å‘Šæç¤ºæ ·å¼ */
	.warning-box {
		background: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
		color: white;
		padding: 1rem;
		border-radius: 10px;
		margin: 1rem 0;
		text-align: center;
		font-weight: 600;
	}
	
	/* æ•°æ®è¡¨æ ¼æ ·å¼ */
	.dataframe {
		border-radius: 10px;
		overflow: hidden;
		box-shadow: 0 4px 16px rgba(0,0,0,0.1);
	}
	
	/* è¿›åº¦æ¡æ ·å¼ */
	.stProgress > div > div > div {
		background: linear-gradient(90deg, #667eea, #764ba2);
	}
	</style>
	""", unsafe_allow_html=True)
	
	# ä½¿ç”¨è‡ªå®šä¹‰æ ·å¼çš„æ ‡é¢˜
	st.markdown('<div class="main-header"><h1>ğŸ“‹ CMSR - ç®€å†ä¿¡æ¯æå–ç³»ç»Ÿ</h1></div>', unsafe_allow_html=True)

	# è·å–APIé…ç½®ï¼ˆé™é»˜è·å–ï¼Œä¸æ˜¾ç¤ºåœ¨ç•Œé¢ä¸Šï¼‰
	api_key, base_url, user_id = get_api_config_from_secrets()
	score_api_key_input = get_score_key_from_secrets()

	if not _HAS_AIA:
		st.warning('æœªæ£€æµ‹åˆ° aiagentplatformpyã€‚è‹¥ä¸ºç§æœ‰åº“ï¼Œäº‘ç«¯æ— æ³•ç›´æ¥å®‰è£…ï¼Œè¯·ä½¿ç”¨å¸¦è¯¥åº“çš„è‡ªå®šä¹‰ç¯å¢ƒæˆ–ç§æœ‰åŒ…é•œåƒï¼›æˆ–è”ç³»ç®¡ç†å‘˜æä¾›å…¬å…±å¯å®‰è£…ç‰ˆæœ¬ã€‚')

	# â€”â€”â€” æ¨¡å¼é€‰æ‹© â€”â€”â€”
	st.markdown('<h3 style="text-align: center; margin: 2rem 0;">ğŸš€ é€‰æ‹©å¤„ç†æ¨¡å¼</h3>', unsafe_allow_html=True)
	
	# ä½¿ç”¨åˆ—å¸ƒå±€åˆ›å»ºæ¨¡å¼é€‰æ‹©å¡ç‰‡
	col1, col2, col3 = st.columns(3)
	
	with col1:
		st.markdown("""
		<div style="text-align: center; padding: 1rem; border-radius: 10px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; margin: 0.5rem;">
			<h4>ğŸ“„ å•æ–‡ä»¶ä¸Šä¼ </h4>
			<p>ä¸Šä¼ Excel/CSV/TXTæ–‡ä»¶</p>
		</div>
		""", unsafe_allow_html=True)
	
	with col2:
		st.markdown("""
		<div style="text-align: center; padding: 1rem; border-radius: 10px; background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); color: white; margin: 0.5rem;">
			<h4>ğŸ“ ä»æ–‡ä»¶åç”Ÿæˆ</h4>
			<p>åŸºäºæ–‡ä»¶åè‡ªåŠ¨ç”ŸæˆæŸ¥è¯¢</p>
		</div>
		""", unsafe_allow_html=True)
	
	with col3:
		st.markdown("""
		<div style="text-align: center; padding: 1rem; border-radius: 10px; background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%); color: white; margin: 0.5rem;">
			<h4>ğŸ“ æ‰‹åŠ¨æ‰¹é‡è¾“å…¥</h4>
			<p>æ‰‹åŠ¨è¾“å…¥æˆ–ç²˜è´´æŸ¥è¯¢</p>
		</div>
		""", unsafe_allow_html=True)
	
	mode = st.radio('é€‰æ‹©æ¨¡å¼ï¼š', ['ğŸ“„ å•æ–‡ä»¶ä¸Šä¼ ', 'ğŸ“ ä»æ–‡ä»¶åç”Ÿæˆ', 'ğŸ“ æ‰‹åŠ¨æ‰¹é‡è¾“å…¥'], horizontal=True, label_visibility="collapsed")

	queries: List[str] = []

	if mode == 'ğŸ“„ å•æ–‡ä»¶ä¸Šä¼ ':
		st.subheader('ğŸ“ ä¸Šä¼ æŸ¥è¯¢æ–‡ä»¶ï¼ˆExcel/CSV/TXTï¼‰')
		uploaded = st.file_uploader('é€‰æ‹©ä¸€ä¸ªåŒ…å«æŸ¥è¯¢åˆ—è¡¨çš„æ–‡ä»¶ï¼š', type=['xlsx', 'xls', 'csv', 'txt'])
		if uploaded is not None:
			with tempfile.NamedTemporaryFile(delete=False, suffix=f".{uploaded.name.split('.')[-1]}") as tmp:
				tmp.write(uploaded.read())
				tmp_path = tmp.name
			loader = QueryLoader()
			queries = loader.load_queries(tmp_path)
			st.success(f'å·²è¯»å– {len(queries)} æ¡æŸ¥è¯¢')
			if queries:
				with st.expander('æŸ¥çœ‹æŸ¥è¯¢é¢„è§ˆ', expanded=False):
					st.write(pd.DataFrame({'æŸ¥è¯¢': queries}))

	elif mode == 'ğŸ“ ä»æ–‡ä»¶åç”Ÿæˆ':
		st.subheader('ğŸ“ é€‰æ‹©å¤šä¸ªæ–‡ä»¶ï¼Œç³»ç»Ÿå°†åŸºäºæ–‡ä»¶åç”ŸæˆæŸ¥è¯¢')
		batch_files = st.file_uploader('é€‰æ‹©å¤šä¸ªä»»æ„ç±»å‹æ–‡ä»¶ï¼šä»…æå–æ–‡ä»¶å', accept_multiple_files=True)
		if batch_files:
			file_names = [bf.name for bf in batch_files]
			extract_queries = [f"{strip_ext(name)}çš„ç®€å†æƒ…å†µ" for name in file_names]
			score_queries = [f"{strip_ext(name)}çš„ç®€å†è¯„åˆ†" for name in file_names]
			queries = extract_queries  # ç”¨äºæå–çš„æŸ¥è¯¢
			st.success(f'å·²ä» {len(file_names)} ä¸ªæ–‡ä»¶åç”Ÿæˆ {len(queries)} æ¡æŸ¥è¯¢')
			with st.expander('æŸ¥çœ‹ç”Ÿæˆçš„æŸ¥è¯¢', expanded=True):
				st.write(pd.DataFrame({
					'æ–‡ä»¶å': file_names, 
					'æå–æŸ¥è¯¢': extract_queries,
					'è¯„åˆ†æŸ¥è¯¢': score_queries
				}))

	else:
		st.subheader('ğŸ“ æ‰‹åŠ¨ç²˜è´´æ‰¹é‡æŸ¥è¯¢ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰')
		text = st.text_area('åœ¨æ­¤ç²˜è´´æˆ–è¾“å…¥ï¼Œæ¯è¡Œä¸€ä¸ªæŸ¥è¯¢ï¼ˆè‡ªåŠ¨è¡¥é½â€œçš„ç®€å†æƒ…å†µ/ç®€å†ä¿¡æ¯â€åç¼€ï¼‰', height=200)
		if text.strip():
			raw = [ln.strip() for ln in text.split('\n') if ln.strip()]
			queries = []
			for q in raw:
				if q.endswith('çš„ç®€å†ä¿¡æ¯') or q.endswith('çš„ç®€å†æƒ…å†µ'):
					queries.append(q)
				else:
					queries.append(q + 'çš„ç®€å†ä¿¡æ¯')
			st.success(f'å…±æ”¶é›† {len(queries)} æ¡æŸ¥è¯¢')
			with st.expander('æŸ¥è¯¢åˆ—è¡¨', expanded=False):
				st.write(pd.DataFrame({'æŸ¥è¯¢': queries}))
			# ç”Ÿæˆå¯ä¸‹è½½çš„TXT
			if queries:
				txt_buf = io.StringIO('\n'.join(queries))
				st.download_button('ğŸ“ ä¸‹è½½æŸ¥è¯¢TXT', data=txt_buf.getvalue().encode('utf-8'), file_name=f"batch_queries_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt", mime='text/plain')

	st.divider()
	can_run = bool(queries)
	run = st.button('ğŸš€ å¼€å§‹æå–ä¸è¯„åˆ†', disabled=not can_run)

	# ä½¿ç”¨ session_state ä¿å­˜é˜¶æ®µæ€§ç»“æœ
	if 'extracted_results' not in st.session_state:
		st.session_state.extracted_results = None
	if 'extracted_failed' not in st.session_state:
		st.session_state.extracted_failed = None
	if 'score_results' not in st.session_state:
		st.session_state.score_results = None
	if 'score_error' not in st.session_state:
		st.session_state.score_error = None
	# åˆå§‹åŒ–æ—¥å¿—å­˜å‚¨å¹¶å¸¸é©»æ˜¾ç¤ºæ—¥å¿—åŒºåŸŸï¼ˆè¿è¡Œç»“æŸåä»å¯è§ï¼‰
	if 'extract_logs' not in st.session_state:
		st.session_state.extract_logs = []
	if 'score_logs' not in st.session_state:
		st.session_state.score_logs = []

	# å¸¸é©»æ—¥å¿—åŒºåŸŸï¼ˆé»˜è®¤å±•å¼€ï¼Œæ˜¾ç¤ºå½“å‰ session_state æ—¥å¿—ï¼‰
	# æå–è¿›åº¦æ¡
	ex_progress_placeholder = st.empty()
	
	ex_log_expander = st.expander('ğŸ“œ æå–æ—¥å¿—', expanded=True)
	with ex_log_expander:
		# æ˜¾ç¤ºå·²æœ‰çš„æå–æ—¥å¿—
		if st.session_state.extract_logs:
			st.text_area(
				label='æå–æ—¥å¿—å†…å®¹',
				value=''.join(st.session_state.extract_logs),
				height=200,
				disabled=True,
				key='extract_log_display'
			)
		ex_log_placeholder = st.empty()

	# è¯„åˆ†è¿›åº¦æ¡
	sc_progress_placeholder = st.empty()
	
	sc_expander = st.expander('ğŸ“œ è¯„åˆ†æ—¥å¿—', expanded=True)
	with sc_expander:
		# æ˜¾ç¤ºå·²æœ‰çš„è¯„åˆ†æ—¥å¿—
		if st.session_state.score_logs:
			st.text_area(
				label='è¯„åˆ†æ—¥å¿—å†…å®¹',
				value=''.join(st.session_state.score_logs),
				height=200,
				disabled=True,
				key='score_log_display'
			)
		sc_placeholder = st.empty()

	# æå–æµç¨‹ä¸è¯„åˆ†æµç¨‹ï¼ˆåˆå¹¶æŒ‰é’®é¡ºåºæ‰§è¡Œï¼‰
	if run:
		with ex_progress_placeholder:
			progress_ex = st.progress(0, text='ğŸš€ æå–å¼€å§‹...')
		# åˆå§‹åŒ–/æ¸…ç©ºæå–æ—¥å¿—
		st.session_state['extract_logs'] = []

		class StreamlitAppendWriter(io.StringIO):
			def write(self, s: str):
				if not s:
					return
				st.session_state['extract_logs'].append(s)
				# æ›´æ–°å›ºå®šçš„æ—¥å¿—æ˜¾ç¤ºåŒºåŸŸ
				ex_log_placeholder.text_area(
					label='å®æ—¶æå–æ—¥å¿—',
					value=''.join(st.session_state['extract_logs']),
					height=200,
					disabled=True,
					key=f'extract_log_realtime_{len(st.session_state["extract_logs"])}'  # åŠ¨æ€key
				)

		extractor = ResumeExtractor(api_key, base_url, user_id)
		# å¤ç”¨æå–ä¼šè¯ID
		if st.session_state.get('extract_conversation_id'):
			extractor.chat_api.conversation_id = st.session_state['extract_conversation_id']
		else:
			conv_id = extractor.chat_api.create_or_load_conversation(use_existing=True)
			st.session_state['extract_conversation_id'] = conv_id
		results = []
		failed = []
		with contextlib.redirect_stdout(StreamlitAppendWriter()):
			for idx, q in enumerate(queries, 1):
				print(f"\n=== å¤„ç†ç¬¬{idx}ä¸ªç®€å†æŸ¥è¯¢ ===")
				print(f"æŸ¥è¯¢: {q}")
				info = extractor.process_resume_query(q)
				if info:
					print("âœ… æˆåŠŸæå–ç®€å†ä¿¡æ¯")
					results.append(info)
				else:
					print("âŒ æå–ç®€å†ä¿¡æ¯å¤±è´¥")
					failed.append({
						'åºå·': idx,
						'æŸ¥è¯¢å†…å®¹': q,
						'å¤±è´¥æ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
						'å¤±è´¥åŸå› ': 'æå–å¤±è´¥æˆ–æ— è¿”å›æ•°æ®æˆ–æ‰€æœ‰å­—æ®µä¸ºç©º'
					})
				progress_ex.progress(int(idx * 100 / len(queries)), text=f'æå–è¿›åº¦ï¼š{idx}/{len(queries)}')
		st.session_state.extracted_results = results
		st.session_state.extracted_failed = failed

	# è¯„åˆ†æµç¨‹
	if run:
		with sc_progress_placeholder:
			progress_sc = st.progress(0, text='ğŸ¯ è¯„åˆ†å¼€å§‹...')
		# åˆå§‹åŒ–/æ¸…ç©ºè¯„åˆ†æ—¥å¿—
		st.session_state['score_logs'] = []

		class StreamlitScoreWriter(io.StringIO):
			def write(self, s: str):
				if not s:
					return
				st.session_state['score_logs'].append(s)
				# æ›´æ–°å›ºå®šçš„æ—¥å¿—æ˜¾ç¤ºåŒºåŸŸ
				sc_placeholder.text_area(
					label='å®æ—¶è¯„åˆ†æ—¥å¿—',
					value=''.join(st.session_state['score_logs']),
					height=200,
					disabled=True,
					key=f'score_log_realtime_{len(st.session_state["score_logs"])}'  # åŠ¨æ€key
				)

		# ä»…ä½¿ç”¨è¯„åˆ†Keyï¼Œä¸ä½¿ç”¨å…œåº•
		use_key = score_api_key_input
		scorer = ResumeScorer(use_key, base_url, user_id)
		# å¤ç”¨è¯„åˆ†ä¼šè¯ID
		if st.session_state.get('score_conversation_id'):
			scorer.chat_api.conversation_id = st.session_state['score_conversation_id']
		else:
			conv_id = scorer.chat_api.create_or_load_conversation(use_existing=True)
			st.session_state['score_conversation_id'] = conv_id
		def to_score_query(q: str) -> str:
			base = str(q).strip()
			for suf in ['çš„ç®€å†ä¿¡æ¯', 'çš„ç®€å†æƒ…å†µ', 'çš„ç®€å†']:
				if base.endswith(suf):
					base = base[:-len(suf)]
					break
			return base + 'çš„ç®€å†è¯„åˆ†'
		score_queries = [to_score_query(q) for q in queries]
		score_data = []
		score_error = None
		with contextlib.redirect_stdout(StreamlitScoreWriter()):
			for idx, q in enumerate(score_queries, 1):
				print(f"\n=== å¤„ç†ç¬¬{idx}ä¸ªè¯„åˆ†æŸ¥è¯¢ ===")
				print(f"æŸ¥è¯¢: {q}")
				try:
					info = scorer.process_score_query(q)
					if info:
						print("âœ… æˆåŠŸè·å–è¯„åˆ†")
					else:
						print("âŒ è¯„åˆ†è¿”å›ç©ºæ•°æ®")
				except Exception as e:
					info = None
					score_error = f'è¯„åˆ†è°ƒç”¨å¤±è´¥: {e}'
					print(f"âŒ è¯„åˆ†å¼‚å¸¸: {e}")
				if info is None and score_error is None:
					score_error = 'è¯„åˆ†è°ƒç”¨å¤±è´¥æˆ–æ— è¿”å›æ•°æ®'
				if info:
					score_data.append(info)
				progress_sc.progress(int(idx * 100 / len(score_queries)), text=f'è¯„åˆ†è¿›åº¦ï¼š{idx}/{len(score_queries)}')
		st.session_state.score_results = score_data
		st.session_state.score_error = score_error

	# å±•ç¤ºæå–ç»“æœ
	if st.session_state.extracted_results is not None:
		results = st.session_state.extracted_results
		failed = st.session_state.extracted_failed or []
		if not results:
			st.markdown('<div class="warning-box">âš ï¸ æ²¡æœ‰æˆåŠŸæå–åˆ°ä»»ä½•ç®€å†æ•°æ®</div>', unsafe_allow_html=True)
		else:
			extractor_tmp = ResumeExtractor(api_key, base_url, user_id)
			extractor_tmp.extracted_data = results
			meta = extractor_tmp.get_extraction_summary()
			
			# ä½¿ç”¨è‡ªå®šä¹‰æ ·å¼çš„æˆåŠŸæç¤º
			st.markdown('<div class="success-box">ğŸ‰ æå–å®Œæˆï¼</div>', unsafe_allow_html=True)
			
			# ç¾åŒ–æŒ‡æ ‡æ˜¾ç¤º
			st.markdown('<h3 style="text-align: center; margin: 2rem 0;">ğŸ“Š æå–ç»Ÿè®¡</h3>', unsafe_allow_html=True)
			
			col1, col2, col3, col4 = st.columns(4)
			with col1:
				st.markdown(f"""
				<div class="metric-card">
					<h2>{meta.get('total_count', 0)}</h2>
					<p>æ€»æå–æ•°é‡</p>
				</div>
				""", unsafe_allow_html=True)
			with col2:
				st.markdown(f"""
				<div class="metric-card">
					<h2>{meta.get('successful_extractions', 0)}</h2>
					<p>æˆåŠŸæå–</p>
				</div>
				""", unsafe_allow_html=True)
			with col3:
				st.markdown(f"""
				<div class="metric-card">
					<h2>{len(meta.get('unique_names', []))}</h2>
					<p>ä¸åŒå§“åæ•°</p>
				</div>
				""", unsafe_allow_html=True)
			with col4:
				st.markdown(f"""
				<div class="metric-card">
					<h2>{len(meta.get('education_levels', []))}</h2>
					<p>å­¦å†ç±»å‹æ•°</p>
				</div>
				""", unsafe_allow_html=True)
			
			# æ·»åŠ æ•°æ®å¯è§†åŒ–
			if meta.get('education_levels'):
				st.markdown('<h4 style="margin: 2rem 0 1rem 0;">ğŸ“ å­¦å†åˆ†å¸ƒ</h4>', unsafe_allow_html=True)
				edu_counts = pd.Series(meta['education_levels']).value_counts()
				col1, col2 = st.columns([2, 1])
				with col1:
					st.bar_chart(edu_counts)
				with col2:
					st.write(edu_counts)
			
			# ç¾åŒ–æ•°æ®è¡¨æ ¼æ˜¾ç¤º
			with st.expander('ğŸ“‹ æŸ¥çœ‹æå–æ˜ç»†ï¼ˆå‰100è¡Œï¼‰', expanded=False):
				df_display = pd.DataFrame(results).head(100)
				st.dataframe(df_display, use_container_width=True, height=400)
			# ä¸‹è½½æå–ç»“æœ
			st.markdown('<h3 style="text-align: center; margin: 2rem 0;">ğŸ“¥ ä¸‹è½½æå–ç»“æœ</h3>', unsafe_allow_html=True)
			ts = datetime.now().strftime('%Y%m%d_%H%M%S')
			
			col1, col2 = st.columns(2)
			with col1:
				excel_bytes = to_excel_bytes(results, sheet_name='ç®€å†ä¿¡æ¯')
				st.download_button('ğŸ“Š ä¸‹è½½ç®€å†Excel', data=excel_bytes, file_name=f"resume_data_{ts}.xlsx", mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')
			
			if failed:
				with col2:
					failed_bytes = to_failed_queries_excel_bytes(failed)
					st.download_button('âš ï¸ ä¸‹è½½å¤±è´¥æŸ¥è¯¢ï¼ˆExcelï¼‰', data=failed_bytes, file_name=f"failed_queries_{ts}.xlsx", mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')

	# å±•ç¤ºè¯„åˆ†ç»“æœ
	if st.session_state.score_results is not None:
		score_data = st.session_state.score_results
		score_error = st.session_state.score_error
		if score_error:
			st.markdown(f'<div class="warning-box">âš ï¸ è¯„åˆ†æç¤ºï¼š{score_error}</div>', unsafe_allow_html=True)
		if score_data:
			# æŒ‰æ€»å¾—åˆ†ä»é«˜åˆ°ä½æ’åº
			df_scores = pd.DataFrame(score_data)
			if 'æ€»å¾—åˆ†' in df_scores.columns:
				df_scores_sorted = df_scores.sort_values('æ€»å¾—åˆ†', ascending=False)
				
				# ä½¿ç”¨è‡ªå®šä¹‰æ ·å¼çš„æˆåŠŸæç¤º
				st.markdown('<div class="success-box">ğŸ¯ è¯„åˆ†å®Œæˆï¼</div>', unsafe_allow_html=True)
				
				# ç¾åŒ–è¯„åˆ†ç»Ÿè®¡æ˜¾ç¤º
				st.markdown('<h3 style="text-align: center; margin: 2rem 0;">ğŸ“Š è¯„åˆ†ç»Ÿè®¡</h3>', unsafe_allow_html=True)
				
				# æ˜¾ç¤ºè¯„åˆ†ç»Ÿè®¡ä¿¡æ¯
				col1, col2, col3, col4 = st.columns(4)
				with col1:
					st.markdown(f"""
					<div class="metric-card">
						<h2>{df_scores_sorted['æ€»å¾—åˆ†'].max()}</h2>
						<p>æœ€é«˜åˆ†</p>
					</div>
					""", unsafe_allow_html=True)
				with col2:
					st.markdown(f"""
					<div class="metric-card">
						<h2>{df_scores_sorted['æ€»å¾—åˆ†'].min()}</h2>
						<p>æœ€ä½åˆ†</p>
					</div>
					""", unsafe_allow_html=True)
				with col3:
					st.markdown(f"""
					<div class="metric-card">
						<h2>{df_scores_sorted['æ€»å¾—åˆ†'].mean():.1f}</h2>
						<p>å¹³å‡åˆ†</p>
					</div>
					""", unsafe_allow_html=True)
				with col4:
					st.markdown(f"""
					<div class="metric-card">
						<h2>{df_scores_sorted['æ€»å¾—åˆ†'].median():.1f}</h2>
						<p>ä¸­ä½æ•°</p>
					</div>
					""", unsafe_allow_html=True)
				
				# æ·»åŠ è¯„åˆ†åˆ†å¸ƒå¯è§†åŒ–
				st.markdown('<h4 style="margin: 2rem 0 1rem 0;">ğŸ“ˆ è¯„åˆ†åˆ†å¸ƒ</h4>', unsafe_allow_html=True)
				col1, col2 = st.columns([2, 1])
				with col1:
					st.bar_chart(df_scores_sorted['æ€»å¾—åˆ†'])
				with col2:
					st.write(f"å…± {len(score_data)} æ¡è¯„åˆ†æ•°æ®")
				
				# æå–æ–‡ä»¶åå¹¶é‡æ–°æ’åˆ—åˆ—é¡ºåº
				if 'è¯„åˆ†æŸ¥è¯¢' in df_scores_sorted.columns:
					# ä»è¯„åˆ†æŸ¥è¯¢ä¸­æå–æ–‡ä»¶å
					def extract_filename(query):
						if pd.isna(query) or not query:
							return ''
						# ç§»é™¤"çš„ç®€å†è¯„åˆ†"åç¼€
						query_str = str(query).strip()
						if query_str.endswith('çš„ç®€å†è¯„åˆ†'):
							return query_str[:-5]  # ç§»é™¤"çš„ç®€å†è¯„åˆ†"
						return query_str
					
					df_scores_sorted['å§“å'] = df_scores_sorted['è¯„åˆ†æŸ¥è¯¢'].apply(extract_filename)
				
				# é‡æ–°æ’åˆ—åˆ—é¡ºåºï¼šæ–‡ä»¶åã€æ€»åˆ†ã€å…¶ä»–å¾—åˆ†é¡¹
				score_columns = list(df_scores_sorted.columns)
				ordered_columns = []
				
				# ç¬¬ä¸€åˆ—ï¼šæ–‡ä»¶å
				if 'å§“å' in score_columns:
					ordered_columns.append('å§“å')
				
				# ç¬¬äºŒåˆ—ï¼šæ€»åˆ†
				if 'æ€»å¾—åˆ†' in score_columns:
					ordered_columns.append('æ€»å¾—åˆ†')
				
				# å…¶ä»–å¾—åˆ†åˆ—ï¼ˆæŒ‰é¡ºåºï¼‰
				score_fields = [
					'æœ¬ç§‘é™¢æ ¡åˆ†', 'ç¡•å£«é™¢æ ¡åˆ†', 'æœ¬ç§‘ä¸“ä¸šç¬¦åˆåº¦åˆ†', 'ç¡•å£«ä¸“ä¸šç¬¦åˆåº¦åˆ†', 
					'äº¤å‰å­¦ç§‘åˆ†', 'å­¦ä¹ æˆç»©åˆ†', 'è‹±è¯­æ°´å¹³åˆ†', 'ç¼–ç¨‹æŠ€èƒ½åˆ†', 
					'é¡¹ç›®å®ä¹ ç»å†åˆ†', 'å­¦ç”Ÿå·¥ä½œåˆ†', 'æŒæ¡CADç±»è½¯ä»¶åŠ åˆ†', 'AVEVA Marineè½¯ä»¶åŠ åˆ†'
				]
				for field in score_fields:
					if field in score_columns:
						ordered_columns.append(field)
				
				# æ·»åŠ å‰©ä½™åˆ—
				for col in score_columns:
					if col not in ordered_columns:
						ordered_columns.append(col)
				
				df_scores_sorted = df_scores_sorted[ordered_columns]
				
				# ç¾åŒ–è¯„åˆ†æ˜ç»†æ˜¾ç¤º
				with st.expander('ğŸ“‹ æŸ¥çœ‹è¯„åˆ†æ˜ç»†ï¼ˆæŒ‰æ€»å¾—åˆ†ä»é«˜åˆ°ä½æ’åºï¼Œå‰100è¡Œï¼‰', expanded=False):
					st.dataframe(df_scores_sorted.head(100), use_container_width=True, height=400)
			else:
				# å¦‚æœæ²¡æœ‰æ€»å¾—åˆ†å­—æ®µï¼ŒæŒ‰åŸæ ·æ˜¾ç¤º
				with st.expander('ğŸ“‹ æŸ¥çœ‹è¯„åˆ†æ˜ç»†ï¼ˆå‰100è¡Œï¼‰', expanded=False):
					st.dataframe(pd.DataFrame(score_data).head(100), use_container_width=True, height=400)
			# ä¸‹è½½è¯„åˆ†ç»“æœ
			st.markdown('<h3 style="text-align: center; margin: 2rem 0;">ğŸ“¥ ä¸‹è½½è¯„åˆ†ç»“æœ</h3>', unsafe_allow_html=True)
			ts = datetime.now().strftime('%Y%m%d_%H%M%S')
			# è‹¥æœ‰æå–ç»“æœï¼Œåˆ™æä¾›åˆå¹¶Excelä¸ZIP
			if st.session_state.extracted_results:
				# å°†è¯„åˆ†æ•°æ®æ‹¼æ¥åˆ°ç®€å†ä¿¡æ¯å³ä¾§
				df_resume = pd.DataFrame(st.session_state.extracted_results)
				df_score = pd.DataFrame(score_data)
				
				# ç”±äºæŸ¥è¯¢æ–‡ä»¶åé¡ºåºä¸€è‡´ï¼Œç›´æ¥æŒ‰ç´¢å¼•åˆå¹¶ï¼ˆæ›´å¯é ï¼‰
				merged_df = pd.concat([df_resume, df_score], axis=1)
				
				# ç”Ÿæˆåˆå¹¶åçš„Excel
				combined_output = io.BytesIO()
				with pd.ExcelWriter(combined_output, engine='openpyxl') as writer:
					merged_df.to_excel(writer, index=False, sheet_name='ç®€å†ä¿¡æ¯ä¸è¯„åˆ†')
				combined_output.seek(0)
				
				col1, col2 = st.columns(2)
				with col1:
					st.download_button('ğŸ“’ ä¸‹è½½åˆå¹¶Excelï¼ˆä¿¡æ¯+è¯„åˆ†ï¼‰', data=combined_output.read(), file_name=f"resume_with_scores_{ts}.xlsx", mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')
				# ç”Ÿæˆè¯„åˆ†JSONæ•°æ®ç”¨äºZIPåŒ…
				scores_json_bytes = json.dumps(score_data, ensure_ascii=False, indent=2).encode('utf-8')
				files_for_zip: List[Tuple[str, bytes]] = [
					(f'resume_with_scores_{ts}.xlsx', combined_output.getvalue()),
					(f'resume_data_{ts}.xlsx', to_excel_bytes(st.session_state.extracted_results)),
					(f'resume_data_{ts}.json', json.dumps(st.session_state.extracted_results, ensure_ascii=False, indent=2).encode('utf-8')),
					(f'resume_scores_{ts}.json', scores_json_bytes)
				]
				if st.session_state.extracted_failed:
					files_for_zip.append((f'failed_queries_{ts}.xlsx', to_failed_queries_excel_bytes(st.session_state.extracted_failed)))
				zip_bytes = build_zip_bytes(files_for_zip)
				with col2:
					st.download_button('ğŸ—œï¸ ä¸‹è½½å…¨éƒ¨ï¼ˆZIPï¼‰', data=zip_bytes, file_name=f"resume_extraction_{ts}.zip", mime='application/zip')


	# é¡µé¢åº•éƒ¨ç¾åŒ–
	st.markdown("---")
	st.markdown("""
	<div style="text-align: center; padding: 2rem; color: #666;">
		<p>ğŸš€ CMSR ç®€å†ä¿¡æ¯æå–ç³»ç»Ÿ | è®©ç®€å†å¤„ç†æ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆ</p>
		<p style="font-size: 0.9rem;">Powered by Streamlit & AI Agent Platform</p>
	</div>
	""", unsafe_allow_html=True)

if __name__ == '__main__':
	main()

import os
import io
import json
import zipfile
import tempfile
from datetime import datetime
from typing import List, Tuple

import streamlit as st
import pandas as pd

from resume_extractor import ResumeExtractor
from resume_scorer import ResumeScorer
from query_loader import QueryLoader

# è¿è¡Œæ—¶æ£€æŸ¥ç¬¬ä¸‰æ–¹å¹³å°SDKæ˜¯å¦å¯ç”¨ï¼Œç»™å‡ºæ›´å‹å¥½çš„æç¤º
try:
    import aiagentplatformpy  # type: ignore
    _HAS_AIA = True
except Exception:
    _HAS_AIA = False


def get_api_config_from_secrets() -> Tuple[str, str, str]:
	api_key = st.secrets.get('RESUME_API_KEY') or st.secrets.get('API_KEY')
	base_url = st.secrets.get('RESUME_BASE_URL') or st.secrets.get('BASE_URL')
	user_id = st.secrets.get('RESUME_USER_ID') or st.secrets.get('USER_ID')
	return api_key, base_url, user_id


def get_score_key_from_secrets() -> str:
	return st.secrets.get('RESUME_SCORE_API_KEY') or st.secrets.get('SCORE_API_KEY') or ''


def strip_ext(filename: str) -> str:
	if '.' not in filename:
		return filename
	return '.'.join(filename.split('.')[:-1])


def to_excel_bytes(data: List[dict], sheet_name: str = 'ç®€å†ä¿¡æ¯') -> bytes:
	if not data:
		return b''
	df = pd.DataFrame(data)
	output = io.BytesIO()
	with pd.ExcelWriter(output, engine='openpyxl') as writer:
		df.to_excel(writer, index=False, sheet_name=sheet_name)
	output.seek(0)
	return output.read()


def to_failed_queries_excel_bytes(failed_queries: List[dict]) -> bytes:
	if not failed_queries:
		return b''
	df = pd.DataFrame(failed_queries)
	output = io.BytesIO()
	with pd.ExcelWriter(output, engine='openpyxl') as writer:
		df.to_excel(writer, index=False, sheet_name='å¤±è´¥æŸ¥è¯¢')
	output.seek(0)
	return output.read()


def build_zip_bytes(files: List[Tuple[str, bytes]]) -> bytes:
	buf = io.BytesIO()
	with zipfile.ZipFile(buf, 'w', zipfile.ZIP_DEFLATED) as zf:
		for name, data in files:
			zf.writestr(name, data)
	buf.seek(0)
	return buf.read()


def main():
	st.set_page_config(page_title='CMSR - ç®€å†ä¿¡æ¯æå–ç³»ç»Ÿ', layout='wide')
	st.title('ğŸ“‹ CMSR - ç®€å†ä¿¡æ¯æå–ç³»ç»Ÿ')

	# â€”â€”â€” ä¾§è¾¹æ ï¼šAPI é…ç½®ï¼ˆæ”¯æŒ Secrets é»˜è®¤ + æ‰‹åŠ¨è¦†ç›–ï¼‰ â€”â€”â€”
	with st.sidebar:
		st.subheader('âš™ï¸ API é…ç½®ï¼ˆä»…ä» Secrets è¯»å–ï¼‰')
		api_key, base_url, user_id = get_api_config_from_secrets()
		score_api_key_input = get_score_key_from_secrets()

		missing = []
		if not api_key: missing.append('RESUME_API_KEY')
		if not base_url: missing.append('RESUME_BASE_URL')
		if not user_id: missing.append('RESUME_USER_ID')
		if missing:
			st.error('æœªé…ç½® Secretsï¼š' + ', '.join(missing))
		else:
			st.success('å·²æ£€æµ‹åˆ° Secrets é…ç½®')

		if not _HAS_AIA:
			st.warning('æœªæ£€æµ‹åˆ° aiagentplatformpyã€‚è‹¥ä¸ºç§æœ‰åº“ï¼Œäº‘ç«¯æ— æ³•ç›´æ¥å®‰è£…ï¼Œè¯·ä½¿ç”¨å¸¦è¯¥åº“çš„è‡ªå®šä¹‰ç¯å¢ƒæˆ–ç§æœ‰åŒ…é•œåƒï¼›æˆ–è”ç³»ç®¡ç†å‘˜æä¾›å…¬å…±å¯å®‰è£…ç‰ˆæœ¬ã€‚')

		st.markdown('---')
		st.subheader('ğŸ“Œ ä½¿ç”¨æç¤º')
		st.markdown('- æ”¯æŒ Excel/CSV/TXT ä¸‰ç§è¾“å…¥æ–¹å¼')
		st.markdown('- ä¹Ÿå¯ä»æ–‡ä»¶åå¿«é€Ÿç”ŸæˆæŸ¥è¯¢æˆ–æ‰‹åŠ¨ç²˜è´´æŸ¥è¯¢')
		st.markdown('- å¤„ç†å®Œæˆåå¯ä¸‹è½½åˆå¹¶Excelã€JSONã€è¯„åˆ†JSONã€å¤±è´¥æŸ¥è¯¢ä¸ZIP')

	# â€”â€”â€” æ¨¡å¼é€‰æ‹© â€”â€”â€”
	mode = st.radio('é€‰æ‹©æ¨¡å¼ï¼š', ['ğŸ“„ å•æ–‡ä»¶ä¸Šä¼ ', 'ğŸ“ ä»æ–‡ä»¶åç”Ÿæˆ', 'ğŸ“ æ‰‹åŠ¨æ‰¹é‡è¾“å…¥'], horizontal=True)

	queries: List[str] = []

	if mode == 'ğŸ“„ å•æ–‡ä»¶ä¸Šä¼ ':
		st.subheader('ğŸ“ ä¸Šä¼ æŸ¥è¯¢æ–‡ä»¶ï¼ˆExcel/CSV/TXTï¼‰')
		uploaded = st.file_uploader('é€‰æ‹©ä¸€ä¸ªåŒ…å«æŸ¥è¯¢åˆ—è¡¨çš„æ–‡ä»¶ï¼š', type=['xlsx', 'xls', 'csv', 'txt'])
		if uploaded is not None:
			with tempfile.NamedTemporaryFile(delete=False, suffix=f".{uploaded.name.split('.')[-1]}") as tmp:
				tmp.write(uploaded.read())
				tmp_path = tmp.name
			loader = QueryLoader()
			queries = loader.load_queries(tmp_path)
			st.success(f'å·²è¯»å– {len(queries)} æ¡æŸ¥è¯¢')
			if queries:
				with st.expander('æŸ¥çœ‹æŸ¥è¯¢é¢„è§ˆ', expanded=False):
					st.write(pd.DataFrame({'æŸ¥è¯¢': queries}))

	elif mode == 'ğŸ“ ä»æ–‡ä»¶åç”Ÿæˆ':
		st.subheader('ğŸ“ é€‰æ‹©å¤šä¸ªæ–‡ä»¶ï¼Œç³»ç»Ÿå°†åŸºäºæ–‡ä»¶åç”ŸæˆæŸ¥è¯¢')
		batch_files = st.file_uploader('é€‰æ‹©å¤šä¸ªä»»æ„ç±»å‹æ–‡ä»¶ï¼šä»…æå–æ–‡ä»¶å', accept_multiple_files=True)
		if batch_files:
			file_names = [bf.name for bf in batch_files]
			queries = [f"{strip_ext(name)}çš„ç®€å†æƒ…å†µ" for name in file_names]
			st.success(f'å·²ä» {len(file_names)} ä¸ªæ–‡ä»¶åç”Ÿæˆ {len(queries)} æ¡æŸ¥è¯¢')
			with st.expander('æŸ¥çœ‹ç”Ÿæˆçš„æŸ¥è¯¢', expanded=True):
				st.write(pd.DataFrame({'æ–‡ä»¶å': file_names, 'ç”Ÿæˆçš„æŸ¥è¯¢': queries}))

	else:
		st.subheader('ğŸ“ æ‰‹åŠ¨ç²˜è´´æ‰¹é‡æŸ¥è¯¢ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰')
		text = st.text_area('åœ¨æ­¤ç²˜è´´æˆ–è¾“å…¥ï¼Œæ¯è¡Œä¸€ä¸ªæŸ¥è¯¢ï¼ˆè‡ªåŠ¨è¡¥é½â€œçš„ç®€å†æƒ…å†µ/ç®€å†ä¿¡æ¯â€åç¼€ï¼‰', height=200)
		if text.strip():
			raw = [ln.strip() for ln in text.split('\n') if ln.strip()]
			queries = []
			for q in raw:
				if q.endswith('çš„ç®€å†ä¿¡æ¯') or q.endswith('çš„ç®€å†æƒ…å†µ'):
					queries.append(q)
				else:
					queries.append(q + 'çš„ç®€å†ä¿¡æ¯')
			st.success(f'å…±æ”¶é›† {len(queries)} æ¡æŸ¥è¯¢')
			with st.expander('æŸ¥è¯¢åˆ—è¡¨', expanded=False):
				st.write(pd.DataFrame({'æŸ¥è¯¢': queries}))
			# ç”Ÿæˆå¯ä¸‹è½½çš„TXT
			if queries:
				txt_buf = io.StringIO('\n'.join(queries))
				st.download_button('ğŸ“ ä¸‹è½½æŸ¥è¯¢TXT', data=txt_buf.getvalue().encode('utf-8'), file_name=f"batch_queries_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt", mime='text/plain')

	st.divider()
	can_extract = bool(queries) and all([api_key, base_url, user_id])
	can_score = bool(queries) and all([(score_api_key_input or api_key), base_url, user_id])
	col_a, col_b = st.columns(2)
	with col_a:
		do_extract = st.button('ğŸš€ å¼€å§‹æå–', disabled=not can_extract)
	with col_b:
		do_score = st.button('ğŸ·ï¸ å¼€å§‹è¯„åˆ†', disabled=not can_score)

	# ä½¿ç”¨ session_state ä¿å­˜é˜¶æ®µæ€§ç»“æœ
	if 'extracted_results' not in st.session_state:
		st.session_state.extracted_results = None
	if 'extracted_failed' not in st.session_state:
		st.session_state.extracted_failed = None
	if 'score_results' not in st.session_state:
		st.session_state.score_results = None
	if 'score_error' not in st.session_state:
		st.session_state.score_error = None

	# æå–æµç¨‹
	if do_extract:
		progress_ex = st.progress(0, text='æå–å¼€å§‹...')
		extractor = ResumeExtractor(api_key, base_url, user_id)
		# å¤ç”¨æå–ä¼šè¯ID
		if st.session_state.get('extract_conversation_id'):
			extractor.chat_api.conversation_id = st.session_state['extract_conversation_id']
		else:
			conv_id = extractor.chat_api.create_or_load_conversation(use_existing=True)
			st.session_state['extract_conversation_id'] = conv_id
		results = []
		failed = []
		for idx, q in enumerate(queries, 1):
			info = extractor.process_resume_query(q)
			if info:
				results.append(info)
			else:
				failed.append({
					'åºå·': idx,
					'æŸ¥è¯¢å†…å®¹': q,
					'å¤±è´¥æ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
					'å¤±è´¥åŸå› ': 'æå–å¤±è´¥æˆ–æ— è¿”å›æ•°æ®æˆ–æ‰€æœ‰å­—æ®µä¸ºç©º'
				})
			progress_ex.progress(int(idx * 100 / len(queries)), text=f'æå–è¿›åº¦ï¼š{idx}/{len(queries)}')
		st.session_state.extracted_results = results
		st.session_state.extracted_failed = failed

	# è¯„åˆ†æµç¨‹
	if do_score:
		progress_sc = st.progress(0, text='è¯„åˆ†å¼€å§‹...')
		use_key = score_api_key_input or api_key
		scorer = ResumeScorer(use_key, base_url, user_id)
		# å¤ç”¨è¯„åˆ†ä¼šè¯ID
		if st.session_state.get('score_conversation_id'):
			scorer.chat_api.conversation_id = st.session_state['score_conversation_id']
		else:
			conv_id = scorer.chat_api.create_or_load_conversation(use_existing=True)
			st.session_state['score_conversation_id'] = conv_id
		def to_score_query(q: str) -> str:
			base = str(q).strip()
			for suf in ['çš„ç®€å†ä¿¡æ¯', 'çš„ç®€å†æƒ…å†µ', 'çš„ç®€å†']:
				if base.endswith(suf):
					base = base[:-len(suf)]
					break
			return base + 'çš„ç®€å†è¯„åˆ†'
		score_queries = [to_score_query(q) for q in queries]
		score_data = []
		score_error = None
		for idx, q in enumerate(score_queries, 1):
			try:
				info = scorer.process_score_query(q)
			except Exception:
				info = None
			if info is None and use_key != api_key:
				# å…œåº•åˆ°æå–Key
				try:
					scorer_fb = ResumeScorer(api_key, base_url, user_id)
					scorer_fb.chat_api.create_or_load_conversation(use_existing=True)
					info = scorer_fb.process_score_query(q)
					score_error = 'è¯„åˆ†APIKeyæ— æ•ˆï¼Œå·²ä½¿ç”¨ç®€å†APIKeyå…œåº•'
				except Exception as e2:
					score_error = f'è¯„åˆ†è°ƒç”¨å¤±è´¥: {e2}'
			if info:
				score_data.append(info)
			progress_sc.progress(int(idx * 100 / len(score_queries)), text=f'è¯„åˆ†è¿›åº¦ï¼š{idx}/{len(score_queries)}')
		st.session_state.score_results = score_data
		st.session_state.score_error = score_error

	# å±•ç¤ºæå–ç»“æœ
	if st.session_state.extracted_results is not None:
		results = st.session_state.extracted_results
		failed = st.session_state.extracted_failed or []
		if not results:
			st.error('æ²¡æœ‰æˆåŠŸæå–åˆ°ä»»ä½•ç®€å†æ•°æ®')
		else:
			extractor_tmp = ResumeExtractor(api_key, base_url, user_id)
			extractor_tmp.extracted_data = results
			meta = extractor_tmp.get_extraction_summary()
			st.success('æå–å®Œæˆï¼')
			col1, col2, col3, col4 = st.columns(4)
			col1.metric('æ€»æå–æ•°é‡', meta.get('total_count', 0))
			col2.metric('æˆåŠŸæå–', meta.get('successful_extractions', 0))
			col3.metric('ä¸åŒå§“åæ•°', len(meta.get('unique_names', [])))
			col4.metric('å­¦å†ç±»å‹æ•°', len(meta.get('education_levels', [])))
			with st.expander('æŸ¥çœ‹æå–æ˜ç»†ï¼ˆå‰100è¡Œï¼‰', expanded=False):
				st.dataframe(pd.DataFrame(results).head(100), use_container_width=True)
			# ä¸‹è½½æå–ç»“æœ
			st.subheader('ğŸ“¥ ä¸‹è½½æå–ç»“æœ')
			ts = datetime.now().strftime('%Y%m%d_%H%M%S')
			excel_bytes = to_excel_bytes(results, sheet_name='ç®€å†ä¿¡æ¯')
			json_bytes = json.dumps(results, ensure_ascii=False, indent=2).encode('utf-8')
			st.download_button('ğŸ“Š ä¸‹è½½ç®€å†Excel', data=excel_bytes, file_name=f"resume_data_{ts}.xlsx", mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')
			st.download_button('ğŸ“„ ä¸‹è½½ç®€å†JSON', data=json_bytes, file_name=f"resume_data_{ts}.json", mime='application/json')
			if failed:
				failed_bytes = to_failed_queries_excel_bytes(failed)
				st.download_button('âš ï¸ ä¸‹è½½å¤±è´¥æŸ¥è¯¢ï¼ˆExcelï¼‰', data=failed_bytes, file_name=f"failed_queries_{ts}.xlsx", mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')

	# å±•ç¤ºè¯„åˆ†ç»“æœ
	if st.session_state.score_results is not None:
		score_data = st.session_state.score_results
		score_error = st.session_state.score_error
		if score_error:
			st.info(f'è¯„åˆ†æç¤ºï¼š{score_error}')
		if score_data:
			with st.expander('æŸ¥çœ‹è¯„åˆ†æ˜ç»†ï¼ˆå‰100è¡Œï¼‰', expanded=False):
				st.dataframe(pd.DataFrame(score_data).head(100), use_container_width=True)
			# ä¸‹è½½è¯„åˆ†ç»“æœ
			st.subheader('ğŸ“¥ ä¸‹è½½è¯„åˆ†ç»“æœ')
			ts = datetime.now().strftime('%Y%m%d_%H%M%S')
			scores_json_bytes = json.dumps(score_data, ensure_ascii=False, indent=2).encode('utf-8')
			st.download_button('ğŸ·ï¸ ä¸‹è½½è¯„åˆ†JSON', data=scores_json_bytes, file_name=f"resume_scores_{ts}.json", mime='application/json')
			# è‹¥æœ‰æå–ç»“æœï¼Œåˆ™æä¾›åˆå¹¶Excelä¸ZIP
			if st.session_state.extracted_results:
				combined_output = io.BytesIO()
				with pd.ExcelWriter(combined_output, engine='openpyxl') as writer:
					pd.DataFrame(st.session_state.extracted_results).to_excel(writer, index=False, sheet_name='ç®€å†ä¿¡æ¯')
					pd.DataFrame(score_data).to_excel(writer, index=False, sheet_name='ç®€å†è¯„åˆ†')
				combined_output.seek(0)
				st.download_button('ğŸ“’ ä¸‹è½½åˆå¹¶Excelï¼ˆå«è¯„åˆ†ï¼‰', data=combined_output.read(), file_name=f"resume_with_scores_{ts}.xlsx", mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')
				files_for_zip: List[Tuple[str, bytes]] = [
					(f'resume_with_scores_{ts}.xlsx', combined_output.getvalue()),
					(f'resume_data_{ts}.xlsx', to_excel_bytes(st.session_state.extracted_results)),
					(f'resume_data_{ts}.json', json.dumps(st.session_state.extracted_results, ensure_ascii=False, indent=2).encode('utf-8')),
					(f'resume_scores_{ts}.json', scores_json_bytes)
				]
				if st.session_state.extracted_failed:
					files_for_zip.append((f'failed_queries_{ts}.xlsx', to_failed_queries_excel_bytes(st.session_state.extracted_failed)))
				zip_bytes = build_zip_bytes(files_for_zip)
				st.download_button('ğŸ—œï¸ ä¸‹è½½å…¨éƒ¨ï¼ˆZIPï¼‰', data=zip_bytes, file_name=f"resume_extraction_{ts}.zip", mime='application/zip')


if __name__ == '__main__':
	main()
